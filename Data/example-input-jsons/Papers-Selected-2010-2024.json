{
  "description": "This contains a curated collection of most influential research papers from 2024 to 2010, covering cutting-edge advancements in AI, machine learning, deep learning, reinforcement learning, and related fields. This store is ideal for retrieving up-to-date academic insights, benchmarking studies, and foundational knowledge in AI. ",
  
  "data": [
    {
      "Year": "2024",
      "Name": "Mixtral of ExpertsIF:7Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: We introduce Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model.",
      "Link": "http://arxiv.org/pdf/2401.04088v1"
    },
    {
      "Year": "2024",
      "Name": "KTO: Model Alignment As Prospect Theoretic OptimizationIF:4Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: Using a Kahneman-Tversky model of human utility, we propose a HALO that directly maximizes the utility of generations instead of maximizing the log-likelihood of preferences, as current methods do.",
      "Link": "http://arxiv.org/pdf/2402.01306v4"
    },
    {
      "Year": "2024",
      "Name": "A Survey of Imitation Learning Methods, Environments and MetricsIF:4Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsViewHighlight: In this survey, we systematically review current imitation learning literature and present our findings by (i) classifying imitation learning techniques, environments and metrics by introducing novel taxonomies; (ii) reflecting on main problems from the literature; and (iii) presenting challenges and future directions for researchers.",
      "Link": "http://arxiv.org/pdf/2404.19456v2"
    },
    {
      "Year": "2024",
      "Name": "Self-Play Fine-Tuning Converts Weak Language Models to Strong Language ModelsIF:4Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: We propose a new fine-tuning method called Self-Play fIne-tuNing (SPIN), which starts from a supervised fine-tuned model.",
      "Link": "http://arxiv.org/pdf/2401.01335v3"
    },
    {
      "Year": "2024",
      "Name": "Human Activity Recognition Using SmartphonesIF:4Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsViewHighlight: Human Activity Recognition is a subject of great research today and has its applications in remote healthcare, activity tracking of the elderly or the disables, calories burnt tracking etc.",
      "Link": "http://arxiv.org/pdf/2404.02869v1"
    },
    {
      "Year": "2024",
      "Name": "KAN: Kolmogorov-Arnold NetworksIF:4Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: Inspired by the Kolmogorov-Arnold representation theorem, we propose Kolmogorov-Arnold Networks (KANs) as promising alternatives to Multi-Layer Perceptrons (MLPs).",
      "Link": "http://arxiv.org/pdf/2404.19756v4"
    },
    {
      "Year": "2024",
      "Name": "HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust RefusalIF:4Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: Automated red teaming holds substantial promise for uncovering and mitigating the risks associated with the malicious use of large language models (LLMs), yet the field lacks a standardized evaluation framework to rigorously assess new methods. To address this issue, we introduce HarmBench, a standardized evaluation framework for automated red teaming.",
      "Link": "http://arxiv.org/pdf/2402.04249v2"
    },
    {
      "Year": "2024",
      "Name": "Length-Controlled AlpacaEval: A Simple Way to Debias Automatic EvaluatorsIF:4Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: We propose a simple regression analysis approach for controlling biases in auto-evaluations.",
      "Link": "http://arxiv.org/pdf/2404.04475v1"
    },
    {
      "Year": "2024",
      "Name": "Medusa: Simple LLM Inference Acceleration Framework with Multiple Decoding HeadsIF:4Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: In this paper, we present Medusa, an efficient method that augments LLM inference by adding extra decoding heads to predict multiple subsequent tokens in parallel.",
      "Link": "http://arxiv.org/pdf/2401.10774v3"
    },
    {
      "Year": "2024",
      "Name": "Transformers Are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space DualityIF:4Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: While Transformers have been the main architecture behind deep learning\u2019s success in language modeling, state-space models (SSMs) such as Mamba have recently been shown to match or outperform Transformers at small to medium scale. We show that these families of models are actually quite closely related, and develop a rich framework of theoretical connections between SSMs and variants of attention, connected through various decompositions of a well-studied class of structured semiseparable matrices.",
      "Link": "http://arxiv.org/pdf/2405.21060v1"
    },
    {
      "Year": "2024",
      "Name": "Sharing Knowledge in Multi-Task Deep Reinforcement LearningIF:4Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: We study the benefit of sharing representations among tasks to enable the effective use of deep neural networks in Multi-Task Reinforcement Learning.",
      "Link": "http://arxiv.org/pdf/2401.09561v1"
    },
    {
      "Year": "2024",
      "Name": "Multiclass Classification Procedure for Detecting Attacks on MQTT-IoT ProtocolIF:3Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsViewHighlight: We have addressed two types of method for classifying the attacks, ensemble methods and deep learning models, more specifically recurrent networks with very satisfactory results.",
      "Link": "http://arxiv.org/pdf/2402.03270v1"
    },
    {
      "Year": "2024",
      "Name": "Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive SurveyIF:3Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsViewHighlight: In this survey, we present comprehensive studies of various PEFT algorithms, examining their performance and computational overhead.",
      "Link": "http://arxiv.org/pdf/2403.14608v7"
    },
    {
      "Year": "2024",
      "Name": "RewardBench: Evaluating Reward Models for Language ModelingIF:3Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: To enhance scientific understanding of reward models, we present RewardBench, a benchmark dataset and code-base for evaluation.",
      "Link": "http://arxiv.org/pdf/2403.13787v2"
    },
    {
      "Year": "2024",
      "Name": "GaLore: Memory-Efficient LLM Training By Gradient Low-Rank ProjectionIF:3Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: In this work, we propose Gradient Low-Rank Projection (GaLore), a training strategy that allows full-parameter learning but is more memory-efficient than common low-rank adaptation methods such as LoRA.",
      "Link": "http://arxiv.org/pdf/2403.03507v2"
    },
    {
      "Year": "2024",
      "Name": "AI and Memory WallIF:3Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsViewHighlight: Here, we analyze encoder and decoder Transformer models and show how memory bandwidth can become the dominant bottleneck for decoder models.",
      "Link": "http://arxiv.org/pdf/2403.14123v1"
    },
    {
      "Year": "2024",
      "Name": "Break The Sequential Dependency of LLM Inference Using Lookahead DecodingIF:3Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: In this paper, we introduce Lookahead decoding, an exact, parallel decoding algorithm that accelerates LLM decoding without needing auxiliary models or data stores.",
      "Link": "http://arxiv.org/pdf/2402.02057v1"
    },
    {
      "Year": "2024",
      "Name": "Helping University Students to Choose Elective Courses By Using A Hybrid Multi-criteria Recommendation System with Genetic OptimizationIF:3Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsViewHighlight: This paper presents a hybrid RS that combines Collaborative Filtering (CF) and Content-based Filtering (CBF) using multiple criteria related both to student and course information to recommend the most suitable courses to the students.",
      "Link": "http://arxiv.org/pdf/2402.08371v1"
    },
    {
      "Year": "2024",
      "Name": "KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache QuantizationIF:3Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: Quantization is a promising approach for compressing KV cache activations; however, existing solutions fail to represent activations accurately in sub-4-bit precision. Our work, KVQuant, facilitates low precision KV cache quantization by incorporating several novel methods: (i) Per-Channel Key Quantization, where we adjust the dimension along which we quantize the Key activations to better match the distribution; (ii) Pre-RoPE Key Quantization, where we quantize Key activations before the rotary positional embedding to mitigate its impact on quantization; (iii) Non-Uniform KV Cache Quantization, where we derive per-layer sensitivity-weighted non-uniform datatypes that better represent the distributions; and (iv) Per-Vector Dense-and-Sparse Quantization, where we isolate outliers separately for each vector to minimize skews in quantization ranges.",
      "Link": "http://arxiv.org/pdf/2401.18079v5"
    },
    {
      "Year": "2024",
      "Name": "Random Forest-Based Prediction of Stroke OutcomeIF:3Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsViewHighlight: We research into the clinical, biochemical and neuroimaging factors associated with the outcome of stroke patients to generate a predictive model using machine learning techniques for prediction of mortality and morbidity 3 months after admission.",
      "Link": "http://arxiv.org/pdf/2402.00638v1"
    },
    {
      "Year": "2024",
      "Name": "Griffin: Mixing Gated Linear Recurrences with Local Attention for Efficient Language ModelsIF:3Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: We propose Hawk, an RNN with gated linear recurrences, and Griffin, a hybrid model that mixes gated linear recurrences with local attention.",
      "Link": "http://arxiv.org/pdf/2402.19427v1"
    },
    {
      "Year": "2024",
      "Name": "VisualWebArena: Evaluating Multimodal Agents on Realistic Visual Web TasksIF:3Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: Given that most computer interfaces cater to human perception, visual information often augments textual data in ways that text-only models struggle to harness effectively. To bridge this gap, we introduce VisualWebArena, a benchmark designed to assess the performance of multimodal web agents on realistic \\textit{visually grounded tasks}.",
      "Link": "http://arxiv.org/pdf/2401.13649v2"
    },
    {
      "Year": "2024",
      "Name": "WARM: On The Benefits of Weight Averaged Reward ModelsIF:3Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsViewHighlight: We identify two primary challenges when designing RMs to mitigate reward hacking: distribution shifts during the RL process and inconsistencies in human preferences. As a solution, we propose Weight Averaged Reward Models (WARM), first fine-tuning multiple RMs, then averaging them in the weight space.",
      "Link": "http://arxiv.org/pdf/2401.12187v1"
    },
    {
      "Year": "2024",
      "Name": "LoRA+: Efficient Low Rank Adaptation of Large ModelsIF:3Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: In this paper, we show that Low Rank Adaptation (LoRA) as originally introduced in Hu et al. (2021) leads to suboptimal finetuning of models with large width (embedding dimension).",
      "Link": "http://arxiv.org/pdf/2402.12354v2"
    },
    {
      "Year": "2024",
      "Name": "Graph-Mamba: Towards Long-Range Graph Sequence Modeling with Selective State SpacesIF:3Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsViewHighlight: In this work, we introduce Graph-Mamba, the first attempt to enhance long-range context modeling in graph networks by integrating a Mamba block with the input-dependent node selection mechanism.",
      "Link": "http://arxiv.org/pdf/2402.00789v1"
    },
    {
      "Year": "2024",
      "Name": "From $r$ to $Q^*$: Your Language Model Is Secretly A Q-FunctionIF:3Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsViewHighlight: We theoretically show that we can derive DPO in the token-level MDP as a general inverse Q-learning algorithm, which satisfies the Bellman equation.",
      "Link": "http://arxiv.org/pdf/2404.12358v2"
    },
    {
      "Year": "2024",
      "Name": "SliceGPT: Compress Large Language Models By Deleting Rows and ColumnsIF:3Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: In this paper we present SliceGPT, a new post-training sparsification scheme which replaces each weight matrix with a smaller (dense) matrix, reducing the embedding dimension of the network.",
      "Link": "http://arxiv.org/pdf/2401.15024v2"
    },
    {
      "Year": "2024",
      "Name": "Robust Prompt Optimization for Defending Language Models Against Jailbreaking AttacksIF:3Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: While some defenses have been proposed, they have not been adapted to newly proposed attacks and more challenging threat models. To address this, we propose an optimization-based objective for defending LLMs against jailbreaking attacks and an algorithm, Robust Prompt Optimization (RPO) to create robust system-level defenses.",
      "Link": "http://arxiv.org/pdf/2401.17263v5"
    },
    {
      "Year": "2024",
      "Name": "A Minimaximalist Approach to Reinforcement Learning from Human FeedbackIF:3Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsViewHighlight: We present Self-Play Preference Optimization (SPO), an algorithm for reinforcement learning from human feedback.",
      "Link": "http://arxiv.org/pdf/2401.04056v2"
    },
    {
      "Year": "2024",
      "Name": "Foundational Challenges in Assuring Alignment and Safety of Large Language ModelsIF:3Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: Based on the identified challenges, we pose $200+$ concrete research questions.",
      "Link": "http://arxiv.org/pdf/2404.09932v2"
    },
    {
      "Year": "2023",
      "Name": "Direct Preference Optimization: Your Language Model Is Secretly A Reward ModelIF:8Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: In this paper we introduce a new parameterization of the reward model in RLHF that enables extraction of the corresponding optimal policy in closed form, allowing us to solve the standard RLHF problem with only a simple classification loss.",
      "Link": "http://arxiv.org/pdf/2305.18290v3"
    },
    {
      "Year": "2023",
      "Name": "QLoRA: Efficient Finetuning of Quantized LLMsIF:8Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: We present QLoRA, an efficient finetuning approach that reduces memory usage enough to finetune a 65B parameter model on a single 48GB GPU while preserving full 16-bit finetuning task performance.",
      "Link": "http://arxiv.org/pdf/2305.14314v1"
    },
    {
      "Year": "2023",
      "Name": "PaLM-E: An Embodied Multimodal Language ModelIF:8Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: We propose embodied language models to directly incorporate real-world continuous sensor modalities into language models and thereby establish the link between words and percepts.",
      "Link": "http://arxiv.org/pdf/2303.03378v1"
    },
    {
      "Year": "2023",
      "Name": "Mamba: Linear-Time Sequence Modeling with Selective State SpacesIF:8Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers\u2019 computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements.",
      "Link": "http://arxiv.org/pdf/2312.00752v2"
    },
    {
      "Year": "2023",
      "Name": "Efficient Memory Management for Large Language Model Serving with PagedAttentionIF:7Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: When managed inefficiently, this memory can be significantly wasted by fragmentation and redundant duplication, limiting the batch size. To address this problem, we propose PagedAttention, an attention algorithm inspired by the classical virtual memory and paging techniques in operating systems.",
      "Link": "http://arxiv.org/pdf/2309.06180v1"
    },
    {
      "Year": "2023",
      "Name": "FlashAttention-2: Faster Attention with Better Parallelism and Work PartitioningIF:7Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: We observe that the inefficiency is due to suboptimal work partitioning between different thread blocks and warps on the GPU, causing either low-occupancy or unnecessary shared memory reads/writes. We propose FlashAttention-2, with better work partitioning to address these issues.",
      "Link": "http://arxiv.org/pdf/2307.08691v1"
    },
    {
      "Year": "2023",
      "Name": "ProlificDreamer: High-Fidelity and Diverse Text-to-3D Generation with Variational Score DistillationIF:7Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: In this work, we propose to model the 3D parameter as a random variable instead of a constant as in SDS and present variational score distillation (VSD), a principled particle-based variational framework to explain and address the aforementioned issues in text-to-3D generation.",
      "Link": "http://arxiv.org/pdf/2305.16213v2"
    },
    {
      "Year": "2023",
      "Name": "BloombergGPT: A Large Language Model for FinanceIF:7Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsViewHighlight: In this work, we present BloombergGPT, a 50 billion parameter language model that is trained on a wide range of financial data.",
      "Link": "http://arxiv.org/pdf/2303.17564v3"
    },
    {
      "Year": "2023",
      "Name": "Consistency ModelsIF:7Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: Diffusion models have made significant breakthroughs in image, audio, and video generation, but they depend on an iterative generation process that causes slow sampling speed and caps their potential for real-time applications. To overcome this limitation, we propose consistency models, a new family of generative models that achieve high sample quality without adversarial training.",
      "Link": "http://arxiv.org/pdf/2303.01469v2"
    },
    {
      "Year": "2023",
      "Name": "Jailbroken: How Does LLM Safety Training Fail?IF:7Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: We hypothesize two failure modes of safety training: competing objectives and mismatched generalization.",
      "Link": "http://arxiv.org/pdf/2307.02483v1"
    },
    {
      "Year": "2023",
      "Name": "SparseGPT: Massive Language Models Can Be Accurately Pruned in One-ShotIF:6Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: We show for the first time that large-scale generative pretrained transformer (GPT) family models can be pruned to at least 50% sparsity in one-shot, without any retraining, at minimal loss of accuracy.",
      "Link": "http://arxiv.org/pdf/2301.00774v3"
    },
    {
      "Year": "2023",
      "Name": "AlpacaFarm: A Simulation Framework for Methods That Learn from Human FeedbackIF:6Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: Replicating and understanding this instruction-following requires tackling three major challenges: the high cost of data collection, the lack of trustworthy evaluation, and the absence of reference method implementations. We address these challenges with AlpacaFarm, a simulator that enables research and development for learning from feedback at a low cost.",
      "Link": "http://arxiv.org/pdf/2305.14387v4"
    },
    {
      "Year": "2023",
      "Name": "Let\u2019s Verify Step By StepIF:6Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: In recent years, large language models have greatly improved in their ability to perform complex multi-step reasoning.",
      "Link": "http://arxiv.org/pdf/2305.20050v1"
    },
    {
      "Year": "2023",
      "Name": "A Watermark for Large Language ModelsIF:6Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: We propose a watermarking framework for proprietary language models.",
      "Link": "http://arxiv.org/pdf/2301.10226v4"
    },
    {
      "Year": "2023",
      "Name": "Jailbreaking Black Box Large Language Models in Twenty QueriesIF:6Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: To this end, we propose Prompt Automatic Iterative Refinement (PAIR), an algorithm that generates semantic jailbreaks with only black-box access to an LLM.",
      "Link": "http://arxiv.org/pdf/2310.08419v4"
    },
    {
      "Year": "2023",
      "Name": "Mathematical Capabilities of ChatGPTIF:5Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: We investigate the mathematical capabilities of two iterations of ChatGPT (released 9-January-2023 and 30-January-2023) and of GPT-4 by testing them on publicly available datasets, as well as hand-crafted ones, using a novel methodology.",
      "Link": "http://arxiv.org/pdf/2301.13867v2"
    },
    {
      "Year": "2023",
      "Name": "A Comprehensive Survey of Continual Learning: Theory, Method and ApplicationIF:5Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: In this work, we present a comprehensive survey of continual learning, seeking to bridge the basic settings, theoretical foundations, representative methods, and practical applications.",
      "Link": "http://arxiv.org/pdf/2302.00487v3"
    },
    {
      "Year": "2023",
      "Name": "Inference-Time Intervention: Eliciting Truthful Answers from A Language ModelIF:5Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: We introduce Inference-Time Intervention (ITI), a technique designed to enhance the truthfulness of large language models (LLMs).",
      "Link": "http://arxiv.org/pdf/2306.03341v6"
    },
    {
      "Year": "2023",
      "Name": "RAFT: Reward RAnked FineTuning for Generative Foundation Model AlignmentIF:5Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: To this end, we introduce a new framework, Reward rAnked FineTuning (RAFT), designed to align generative models effectively.",
      "Link": "http://arxiv.org/pdf/2304.06767v4"
    },
    {
      "Year": "2023",
      "Name": "Progress Measures for Grokking Via Mechanistic InterpretabilityIF:5Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: We confirm the algorithm by analyzing the activations and weights and by performing ablations in Fourier space.",
      "Link": "http://arxiv.org/pdf/2301.05217v3"
    },
    {
      "Year": "2023",
      "Name": "Habits and Goals in Synergy: A Variational Bayesian Framework for BehaviorIF:5Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: The habitual behavior is generated by using prior distribution of intention, which is goal-less; and the goal-directed behavior is generated by the posterior distribution of intention, which is conditioned on the goal. Building on this idea, we present a novel Bayesian framework for modeling behaviors.",
      "Link": "http://arxiv.org/pdf/2304.05008v1"
    },
    {
      "Year": "2023",
      "Name": "Zephyr: Direct Distillation of LM AlignmentIF:5Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: We aim to produce a smaller language model that is aligned to user intent.",
      "Link": "http://arxiv.org/pdf/2310.16944v1"
    },
    {
      "Year": "2023",
      "Name": "CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Benchmarking on HumanEval-XIF:5Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: In this paper, we introduce CodeGeeX, a multilingual model with 13 billion parameters for code generation.",
      "Link": "http://arxiv.org/pdf/2303.17568v2"
    },
    {
      "Year": "2023",
      "Name": "Large Language Models As OptimizersIF:5Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: In this work, we propose Optimization by PROmpting (OPRO), a simple and effective approach to leverage large language models (LLMs) as optimizers, where the optimization task is described in natural language.",
      "Link": "http://arxiv.org/pdf/2309.03409v3"
    },
    {
      "Year": "2023",
      "Name": "Symbolic Discovery of Optimization AlgorithmsIF:5Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: We present a method to formulate algorithm discovery as program search, and apply it to discover optimization algorithms for deep neural network training.",
      "Link": "http://arxiv.org/pdf/2302.06675v4"
    },
    {
      "Year": "2023",
      "Name": "Principle-Driven Self-Alignment of Language Models from Scratch with Minimal Human SupervisionIF:5Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: However, this dependence can significantly constrain the true potential of AI-assistant agents due to the high cost of obtaining human supervision and the related issues on quality, reliability, diversity, self-consistency, and undesirable biases. To address these challenges, we propose a novel approach called SELF-ALIGN, which combines principle-driven reasoning and the generative power of LLMs for the self-alignment of AI agents with minimal human supervision.",
      "Link": "http://arxiv.org/pdf/2305.03047v2"
    },
    {
      "Year": "2023",
      "Name": "A Cookbook of Self-Supervised LearningIF:5Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsViewHighlight: Our goal is to lower the barrier to entry into SSL research by laying the foundations and latest SSL recipes in the style of a cookbook.",
      "Link": "http://arxiv.org/pdf/2304.12210v2"
    },
    {
      "Year": "2023",
      "Name": "Scalable Extraction of Training Data from (Production) Language ModelsIF:5Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsViewHighlight: This paper studies extractable memorization: training data that an adversary can efficiently extract by querying a machine learning model without prior knowledge of the training dataset.",
      "Link": "http://arxiv.org/pdf/2311.17035v1"
    },
    {
      "Year": "2023",
      "Name": "Representation Engineering: A Top-Down Approach to AI TransparencyIF:4Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: In this paper, we identify and characterize the emerging area of representation engineering (RepE), an approach to enhancing the transparency of AI systems that draws on insights from cognitive neuroscience.",
      "Link": "http://arxiv.org/pdf/2310.01405v3"
    },
    {
      "Year": "2023",
      "Name": "Baseline Defenses for Adversarial Attacks Against Aligned Language ModelsIF:4Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: Recent work shows that text optimizers can produce jailbreaking prompts that bypass moderation and alignment. Drawing from the rich body of work on adversarial machine learning, we approach these attacks with three questions: What threat models are practically useful in this domain?",
      "Link": "http://arxiv.org/pdf/2309.00614v2"
    },
    {
      "Year": "2022",
      "Name": "Classifier-Free Diffusion GuidanceIF:8Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: It also raises the question of whether guidance can be performed without a classifier. We show that guidance can be indeed performed by a pure generative model without such a classifier: in what we call classifier-free guidance, we jointly train a conditional and an unconditional diffusion model, and we combine the resulting conditional and unconditional score estimates to attain a trade-off between sample quality and diversity similar to that obtained using classifier guidance.",
      "Link": "http://arxiv.org/pdf/2207.12598v1"
    },
    {
      "Year": "2022",
      "Name": "Scaling Instruction-Finetuned Language ModelsIF:8Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: In this paper we explore instruction finetuning with a particular focus on (1) scaling the number of tasks, (2) scaling the model size, and (3) finetuning on chain-of-thought data.",
      "Link": "http://arxiv.org/pdf/2210.11416v5"
    },
    {
      "Year": "2022",
      "Name": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-AwarenessIF:8Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: We propose FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM.",
      "Link": "http://arxiv.org/pdf/2205.14135v2"
    },
    {
      "Year": "2022",
      "Name": "DPM-Solver: A Fast ODE Solver for Diffusion Probabilistic Model Sampling in Around 10 StepsIF:7Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: In this work, we propose an exact formulation of the solution of diffusion ODEs.",
      "Link": "http://arxiv.org/pdf/2206.00927v3"
    },
    {
      "Year": "2022",
      "Name": "Diffusion Models: A Comprehensive Survey of Methods and ApplicationsIF:7Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: In this survey, we provide an overview of the rapidly expanding body of work on diffusion models, categorizing the research into three key areas: efficient sampling, improved likelihood estimation, and handling data with special structures.",
      "Link": "http://arxiv.org/pdf/2209.00796v14"
    },
    {
      "Year": "2022",
      "Name": "FEDformer: Frequency Enhanced Decomposed Transformer for Long-term Series ForecastingIF:7Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: Although Transformer-based methods have significantly improved state-of-the-art results for long-term series forecasting, they are not only computationally expensive but more importantly, are unable to capture the global view of time series (e.g. overall trend). To address these problems, we propose to combine Transformer with the seasonal-trend decomposition method, in which the decomposition method captures the global profile of time series while Transformers capture more detailed structures.",
      "Link": "http://arxiv.org/pdf/2201.12740v3"
    },
    {
      "Year": "2022",
      "Name": "Language Models As Zero-Shot Planners: Extracting Actionable Knowledge for Embodied AgentsIF:7Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: In this paper, we investigate the possibility of grounding high-level tasks, expressed in natural language (e.g. make breakfast), to a chosen set of actionable steps (e.g. open fridge).",
      "Link": "http://arxiv.org/pdf/2201.07207v2"
    },
    {
      "Year": "2022",
      "Name": "CodeGen: An Open Large Language Model for Code with Multi-Turn Program SynthesisIF:7Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: The prevalence of large language models advances the state-of-the-art for program synthesis, though limited training resources and data impede open access to such models. To democratize this, we train and release a family of large language models up to 16.1B parameters, called CODEGEN, on natural language and programming language data, and open source the training library JAXFORMER.",
      "Link": "http://arxiv.org/pdf/2203.13474v5"
    },
    {
      "Year": "2022",
      "Name": "Data2vec: A General Framework for Self-supervised Learning in Speech, Vision and LanguageIF:7Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: To get us closer to general self-supervised learning, we present data2vec, a framework that uses the same learning method for either speech, NLP or computer vision.",
      "Link": "http://arxiv.org/pdf/2202.03555v3"
    },
    {
      "Year": "2022",
      "Name": "Model Soups: Averaging Weights of Multiple Fine-tuned Models Improves Accuracy Without Increasing Inference TimeIF:7Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: The conventional recipe for maximizing model accuracy is to (1) train multiple models with various hyperparameters and (2) pick the individual model which performs best on a held-out validation set, discarding the remainder. In this paper, we revisit the second step of this procedure in the context of fine-tuning large pre-trained models, where fine-tuned models often appear to lie in a single low error basin.",
      "Link": "http://arxiv.org/pdf/2203.05482v3"
    },
    {
      "Year": "2022",
      "Name": "Large Language Models Are Human-Level Prompt EngineersIF:7Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: Inspired by classical program synthesis and the human approach to prompt engineering, we propose Automatic Prompt Engineer (APE) for automatic instruction generation and selection.",
      "Link": "http://arxiv.org/pdf/2211.01910v2"
    },
    {
      "Year": "2022",
      "Name": "Few-Shot Parameter-Efficient Fine-Tuning Is Better and Cheaper Than In-Context LearningIF:7Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: Along the way, we introduce a new PEFT method called (IA)$^3$ that scales activations by learned vectors, attaining stronger performance while only introducing a relatively tiny amount of new parameters.",
      "Link": "http://arxiv.org/pdf/2205.05638v2"
    },
    {
      "Year": "2022",
      "Name": "A Time Series Is Worth 64 Words: Long-term Forecasting with TransformersIF:7Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: We propose an efficient design of Transformer-based models for multivariate time series forecasting and self-supervised representation learning.",
      "Link": "http://arxiv.org/pdf/2211.14730v2"
    },
    {
      "Year": "2022",
      "Name": "GPTQ: Accurate Post-Training Quantization for Generative Pre-trained TransformersIF:7Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: While there is emerging work on relieving this pressure via model compression, the applicability and performance of existing compression techniques is limited by the scale and complexity of GPT models. In this paper, we address this challenge, and propose GPTQ, a new one-shot weight quantization method based on approximate second-order information, that is both highly-accurate and highly-efficient.",
      "Link": "http://arxiv.org/pdf/2210.17323v2"
    },
    {
      "Year": "2022",
      "Name": "Transformers in Time Series: A SurveyIF:6Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: In this paper, we systematically review Transformer schemes for time series modeling by highlighting their strengths as well as limitations.",
      "Link": "http://arxiv.org/pdf/2202.07125v5"
    },
    {
      "Year": "2022",
      "Name": "LLM.int8(): 8-bit Matrix Multiplication for Transformers at ScaleIF:6Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: Large language models have been widely adopted but require significant GPU memory for inference.",
      "Link": "http://arxiv.org/pdf/2208.07339v2"
    },
    {
      "Year": "2022",
      "Name": "Reproducible Scaling Laws for Contrastive Language-image LearningIF:6Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: However, previous work on scaling laws has primarily used private data \\& models or focused on uni-modal language or vision learning. To address these limitations, we investigate scaling laws for contrastive language-image pre-training (CLIP) with the public LAION dataset and the open-source OpenCLIP repository.",
      "Link": "http://arxiv.org/pdf/2212.07143v2"
    },
    {
      "Year": "2022",
      "Name": "Quantifying Memorization Across Neural Language ModelsIF:6Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: We describe three log-linear relationships that quantify the degree to which LMs emit memorized training data.",
      "Link": "http://arxiv.org/pdf/2202.07646v3"
    },
    {
      "Year": "2022",
      "Name": "Improving Alignment of Dialogue Agents Via Targeted Human JudgementsIF:6Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsViewHighlight: We present Sparrow, an information-seeking dialogue agent trained to be more helpful, correct, and harmless compared to prompted language model baselines.",
      "Link": "http://arxiv.org/pdf/2209.14375v1"
    },
    {
      "Year": "2022",
      "Name": "Equivariant Diffusion for Molecule Generation in 3DIF:6Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: This work introduces a diffusion model for molecule generation in 3D that is equivariant to Euclidean transformations.",
      "Link": "http://arxiv.org/pdf/2203.17003v2"
    },
    {
      "Year": "2022",
      "Name": "Flow Matching for Generative ModelingIF:6Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: We introduce a new paradigm for generative modeling built on Continuous Normalizing Flows (CNFs), allowing us to train CNFs at unprecedented scale.",
      "Link": "http://arxiv.org/pdf/2210.02747v2"
    },
    {
      "Year": "2022",
      "Name": "Planning with Diffusion for Flexible Behavior SynthesisIF:6Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: While conceptually simple, this combination has a number of empirical shortcomings, suggesting that learned models may not be well-suited to standard trajectory optimization. In this paper, we consider what it would look like to fold as much of the trajectory optimization pipeline as possible into the modeling problem, such that sampling from the model and planning with it become nearly identical.",
      "Link": "http://arxiv.org/pdf/2205.09991v2"
    },
    {
      "Year": "2022",
      "Name": "StyleGAN-XL: Scaling StyleGAN to Large Diverse DatasetsIF:6Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: We demonstrate that this model can invert and edit images beyond the narrow domain of portraits or specific object classes.",
      "Link": "http://arxiv.org/pdf/2202.00273v2"
    },
    {
      "Year": "2022",
      "Name": "Recipe for A General, Powerful, Scalable Graph TransformerIF:6Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: Graph Transformers (GTs) have gained popularity in the field of graph representation learning with a variety of recent publications but they lack a common foundation about what constitutes a good positional or structural encoding, and what differentiates them. In this paper, we summarize the different types of encodings with a clearer definition and categorize them as being $\\textit{local}$, $\\textit{global}$ or $\\textit{relative}$.",
      "Link": "http://arxiv.org/pdf/2205.12454v4"
    },
    {
      "Year": "2022",
      "Name": "TimesNet: Temporal 2D-Variation Modeling for General Time Series AnalysisIF:6Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: Technically, we propose the TimesNet with TimesBlock as a task-general backbone for time series analysis.",
      "Link": "http://arxiv.org/pdf/2210.02186v3"
    },
    {
      "Year": "2022",
      "Name": "CoST: Contrastive Learning of Disentangled Seasonal-Trend Representations for Time Series ForecastingIF:6Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: Motivated by the recent success of representation learning in computer vision and natural language processing, we argue that a more promising paradigm for time series forecasting, is to first learn disentangled feature representations, followed by a simple regression fine-tuning step \u2014 we justify such a paradigm from a causal perspective. Following this principle, we propose a new time series representation learning framework for time series forecasting named CoST, which applies contrastive learning methods to learn disentangled seasonal-trend representations.",
      "Link": "http://arxiv.org/pdf/2202.01575v3"
    },
    {
      "Year": "2022",
      "Name": "DPM-Solver++: Fast Solver for Guided Sampling of Diffusion Probabilistic ModelsIF:6Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: Although recent works propose dedicated high-order solvers and achieve a further speedup for sampling without guidance, their effectiveness for guided sampling has not been well-tested before. In this work, we demonstrate that previous high-order fast samplers suffer from instability issues, and they even become slower than DDIM when the guidance scale grows large.",
      "Link": "http://arxiv.org/pdf/2211.01095v2"
    },
    {
      "Year": "2022",
      "Name": "Out-of-Distribution Detection with Deep Nearest NeighborsIF:6Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: In this paper, we explore the efficacy of non-parametric nearest-neighbor distance for OOD detection, which has been largely overlooked in the literature.",
      "Link": "http://arxiv.org/pdf/2204.06507v3"
    },
    {
      "Year": "2022",
      "Name": "GraphMAE: Self-Supervised Masked Graph AutoencodersIF:6Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: In this paper, we identify and examine the issues that negatively impact the development of GAEs, including their reconstruction objective, training robustness, and error metric.",
      "Link": "http://arxiv.org/pdf/2205.10803v3"
    },
    {
      "Year": "2022",
      "Name": "Out of One, Many: Using Language Models to Simulate Human SamplesIF:6Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsViewHighlight: We propose and explore the possibility that language models can be studied as effective proxies for specific human sub-populations in social science research.",
      "Link": "http://arxiv.org/pdf/2209.06899v1"
    },
    {
      "Year": "2021",
      "Name": "Diffusion Models Beat GANs on Image SynthesisIF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: We show that diffusion models can achieve image sample quality superior to the current state-of-the-art generative models.",
      "Link": "http://arxiv.org/pdf/2105.05233v4"
    },
    {
      "Year": "2021",
      "Name": "Evaluating Large Language Models Trained on CodeIF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: We introduce Codex, a GPT language model fine-tuned on publicly available code from GitHub, and study its Python code-writing capabilities.",
      "Link": "http://arxiv.org/pdf/2107.03374v2"
    },
    {
      "Year": "2021",
      "Name": "On The Opportunities and Risks of Foundation ModelsIF:8Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: AI is undergoing a paradigm shift with the rise of models (e.g., BERT, DALL-E, GPT-3) that are trained on broad data at scale and are adaptable to a wide range of downstream tasks. We call these models foundation models to underscore their critically central yet incomplete character.",
      "Link": "http://arxiv.org/pdf/2108.07258v3"
    },
    {
      "Year": "2021",
      "Name": "Improved Denoising Diffusion Probabilistic ModelsIF:8Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: We show that with a few simple modifications, DDPMs can also achieve competitive log-likelihoods while maintaining high sample quality.",
      "Link": "http://arxiv.org/pdf/2102.09672v1"
    },
    {
      "Year": "2021",
      "Name": "Training Verifiers to Solve Math Word ProblemsIF:8Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: To increase performance, we propose training verifiers to judge the correctness of model completions.",
      "Link": "http://arxiv.org/pdf/2110.14168v2"
    },
    {
      "Year": "2021",
      "Name": "Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient SparsityIF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: We design models based off T5-Base and T5-Large to obtain up to 7x increases in pre-training speed with the same computational resources.",
      "Link": "http://arxiv.org/pdf/2101.03961v3"
    },
    {
      "Year": "2021",
      "Name": "Multitask Prompted Training Enables Zero-Shot Task GeneralizationIF:8Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: Large language models have recently been shown to attain reasonable zero-shot generalization on a diverse set of tasks (Brown et al., 2020).",
      "Link": "http://arxiv.org/pdf/2110.08207v3"
    },
    {
      "Year": "2021",
      "Name": "Autoformer: Decomposition Transformers with Auto-Correlation for Long-Term Series ForecastingIF:8Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: This paper studies the long-term forecasting problem of time series.",
      "Link": "http://arxiv.org/pdf/2106.13008v5"
    },
    {
      "Year": "2021",
      "Name": "Decision Transformer: Reinforcement Learning Via Sequence ModelingIF:8Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: We introduce a framework that abstracts Reinforcement Learning (RL) as a sequence modeling problem.",
      "Link": "http://arxiv.org/pdf/2106.01345v2"
    },
    {
      "Year": "2021",
      "Name": "Efficiently Modeling Long Sequences with Structured State SpacesIF:7Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: We propose the Structured State Space sequence model (S4) based on a new parameterization for the SSM, and show that it can be computed much more efficiently than prior approaches while preserving their theoretical strengths.",
      "Link": "http://arxiv.org/pdf/2111.00396v3"
    },
    {
      "Year": "2021",
      "Name": "Geometric Deep Learning: Grids, Groups, Graphs, Geodesics, and GaugesIF:7Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: Such a \u2018geometric unification\u2019 endeavour, in the spirit of Felix Klein\u2019s Erlangen Program, serves a dual purpose: on one hand, it provides a common mathematical framework to study the most successful neural network architectures, such as CNNs, RNNs, GNNs, and Transformers.",
      "Link": "http://arxiv.org/pdf/2104.13478v2"
    },
    {
      "Year": "2021",
      "Name": "Measuring Mathematical Problem Solving With The MATH DatasetIF:7Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: To measure this ability in machine learning models, we introduce MATH, a new dataset of 12,500 challenging competition mathematics problems.",
      "Link": "http://arxiv.org/pdf/2103.03874v2"
    },
    {
      "Year": "2021",
      "Name": "Generalizing to Unseen Domains: A Survey on Domain GeneralizationIF:7Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: Great progress has been made in the area of domain generalization for years. This paper presents the first review of recent advances in this area.",
      "Link": "http://arxiv.org/pdf/2103.03097v7"
    },
    {
      "Year": "2021",
      "Name": "Ensemble Deep Learning: A ReviewIF:7Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsViewHighlight: This paper reviews the state-of-art deep ensemble models and hence serves as an extensive summary for the researchers.",
      "Link": "http://arxiv.org/pdf/2104.02395v3"
    },
    {
      "Year": "2021",
      "Name": "Tabular Data: Deep Learning Is Not All You NeedIF:7Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: Our study shows that XGBoost outperforms these deep models across the datasets, including the datasets used in the papers that proposed the deep models.",
      "Link": "http://arxiv.org/pdf/2106.03253v2"
    },
    {
      "Year": "2021",
      "Name": "The Surprising Effectiveness of PPO in Cooperative, Multi-Agent GamesIF:7Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: In this work, we carefully study the performance of PPO in cooperative multi-agent settings.",
      "Link": "http://arxiv.org/pdf/2103.01955v4"
    },
    {
      "Year": "2021",
      "Name": "Variational Diffusion ModelsIF:8Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: Diffusion-based generative models have demonstrated a capacity for perceptually impressive synthesis, but can they also be great likelihood-based models? We answer this in the affirmative, and introduce a family of diffusion-based generative models that obtain state-of-the-art likelihoods on standard image density estimation benchmarks.",
      "Link": "http://arxiv.org/pdf/2107.00630v6"
    },
    {
      "Year": "2021",
      "Name": "A Survey of Uncertainty in Deep Neural NetworksIF:7Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsViewHighlight: For a practical application, we discuss different measures of uncertainty, approaches for the calibration of neural networks and give an overview of existing baselines and implementations.",
      "Link": "http://arxiv.org/pdf/2107.03342v3"
    },
    {
      "Year": "2021",
      "Name": "A Survey of TransformersIF:7Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: In this survey, we provide a comprehensive review of various X-formers.",
      "Link": "http://arxiv.org/pdf/2106.04554v2"
    },
    {
      "Year": "2021",
      "Name": "How Attentive Are Graph Attention Networks?IF:7Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: To remove this limitation, we introduce a simple fix by modifying the order of operations and propose GATv2: a dynamic graph attention variant that is strictly more expressive than GAT.",
      "Link": "http://arxiv.org/pdf/2105.14491v3"
    },
    {
      "Year": "2021",
      "Name": "E(n) Equivariant Graph Neural NetworksIF:8Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: This paper introduces a new model to learn graph neural networks equivariant to rotations, translations, reflections and permutations called E(n)-Equivariant Graph Neural Networks (EGNNs).",
      "Link": "http://arxiv.org/pdf/2102.09844v3"
    },
    {
      "Year": "2021",
      "Name": "Model-Contrastive Federated LearningIF:7Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: In this paper, we propose MOON: model-contrastive federated learning.",
      "Link": "http://arxiv.org/pdf/2103.16257v1"
    },
    {
      "Year": "2021",
      "Name": "Domain Generalization: A SurveyIF:7Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: In this paper, for the first time a comprehensive literature review in DG is provided to summarize the developments over the past decade.",
      "Link": "http://arxiv.org/pdf/2103.02503v7"
    },
    {
      "Year": "2021",
      "Name": "Federated Learning on Non-IID Data Silos: An Experimental StudyIF:7Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: In this paper, to help researchers better understand and study the non-IID data setting in federated learning, we propose comprehensive data partitioning strategies to cover the typical non-IID data cases.",
      "Link": "http://arxiv.org/pdf/2102.02079v4"
    },
    {
      "Year": "2021",
      "Name": "FlexMatch: Boosting Semi-Supervised Learning with Curriculum Pseudo LabelingIF:7Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: To address this issue, we propose Curriculum Pseudo Labeling (CPL), a curriculum learning approach to leverage unlabeled data according to the model\u2019s learning status.",
      "Link": "http://arxiv.org/pdf/2110.08263v3"
    },
    {
      "Year": "2021",
      "Name": "FedBN: Federated Learning on Non-IID Features Via Local Batch NormalizationIF:7Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: In this work, we propose an effective method that uses local batch normalization to alleviate the feature shift before averaging models.",
      "Link": "http://arxiv.org/pdf/2102.07623v2"
    },
    {
      "Year": "2021",
      "Name": "Offline Reinforcement Learning with Implicit Q-LearningIF:7Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: We propose an offline RL method that never needs to evaluate actions outside of the dataset, but still enables the learned policy to improve substantially over the best behavior in the data through generalization.",
      "Link": "http://arxiv.org/pdf/2110.06169v1"
    },
    {
      "Year": "2021",
      "Name": "Graph Neural Network-Based Anomaly Detection in Multivariate Time SeriesIF:7Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: Given high-dimensional time series data (e.g., sensor data), how can we detect anomalous events, such as system faults and attacks?",
      "Link": "http://arxiv.org/pdf/2106.06947v1"
    },
    {
      "Year": "2021",
      "Name": "A Minimalist Approach to Offline Reinforcement LearningIF:7Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: In this paper we aim to make a deep RL algorithm work while making minimal changes.",
      "Link": "http://arxiv.org/pdf/2106.06860v2"
    },
    {
      "Year": "2021",
      "Name": "Towards Personalized Federated LearningIF:7Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsViewHighlight: In this survey, we explore the domain of Personalized FL (PFL) to address the fundamental challenges of FL on heterogeneous data, a universal characteristic inherent in all real-world datasets.",
      "Link": "http://arxiv.org/pdf/2103.00710v3"
    },
    {
      "Year": "2020",
      "Name": "A Simple Framework For Contrastive Learning Of Visual RepresentationsIF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: This paper presents SimCLR: a simple framework for contrastive learning of visual representations.",
      "Link": "http://arxiv.org/pdf/2002.05709v3"
    },
    {
      "Year": "2020",
      "Name": "Denoising Diffusion Probabilistic ModelsIF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics.",
      "Link": "http://arxiv.org/pdf/2006.11239v2"
    },
    {
      "Year": "2020",
      "Name": "Bootstrap Your Own Latent: A New Approach To Self-supervised LearningIF:8Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: We introduce Bootstrap Your Own Latent (BYOL), a new approach to self-supervised image representation learning.",
      "Link": "http://arxiv.org/pdf/2006.07733v3"
    },
    {
      "Year": "2020",
      "Name": "Denoising Diffusion Implicit ModelsIF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: To accelerate sampling, we present denoising diffusion implicit models (DDIMs), a more efficient class of iterative implicit probabilistic models with the same training procedure as DDPMs.",
      "Link": "http://arxiv.org/pdf/2010.02502v4"
    },
    {
      "Year": "2020",
      "Name": "Score-Based Generative Modeling Through Stochastic Differential EquationsIF:8Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: In particular, we introduce a predictor-corrector framework to correct errors in the evolution of the discretized reverse-time SDE.",
      "Link": "http://arxiv.org/pdf/2011.13456v2"
    },
    {
      "Year": "2020",
      "Name": "Supervised Contrastive LearningIF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: In this work, we extend the self-supervised batch contrastive approach to the fully-supervised setting, allowing us to effectively leverage label information.",
      "Link": "http://arxiv.org/pdf/2004.11362v5"
    },
    {
      "Year": "2020",
      "Name": "A Tutorial on Learning With Bayesian NetworksIF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: With regard to the latter task, we describe methods for learning both the parameters and structure of a Bayesian network, including techniques for learning with incomplete data.",
      "Link": "http://arxiv.org/pdf/2002.00269v3"
    },
    {
      "Year": "2020",
      "Name": "Scaling Laws For Neural Language ModelsIF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: We study empirical scaling laws for language model performance on the cross-entropy loss.",
      "Link": "http://arxiv.org/pdf/2001.08361v1"
    },
    {
      "Year": "2020",
      "Name": "FixMatch: Simplifying Semi-Supervised Learning With Consistency And ConfidenceIF:8Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: In this paper, we demonstrate the power of a simple combination of two common SSL methods: consistency regularization and pseudo-labeling.",
      "Link": "http://arxiv.org/pdf/2001.07685v2"
    },
    {
      "Year": "2020",
      "Name": "Informer: Beyond Efficient Transformer for Long Sequence Time-Series ForecastingIF:8Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: To address these issues, we design an efficient transformer-based model for LSTF, named Informer, with three distinctive characteristics: (i) a $ProbSparse$ self-attention mechanism, which achieves $O(L \\log L)$ in time complexity and memory usage, and has comparable performance on sequences\u2019 dependency alignment.",
      "Link": "http://arxiv.org/pdf/2012.07436v3"
    },
    {
      "Year": "2020",
      "Name": "Open Graph Benchmark: Datasets for Machine Learning on GraphsIF:8Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: We present the Open Graph Benchmark (OGB), a diverse set of challenging and realistic benchmark datasets to facilitate scalable, robust, and reproducible graph machine learning (ML) research.",
      "Link": "http://arxiv.org/pdf/2005.00687v7"
    },
    {
      "Year": "2020",
      "Name": "Knowledge Distillation: A SurveyIF:8Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsViewHighlight: As a representative type of model compression and acceleration, knowledge distillation effectively learns a small student model from a large teacher model.",
      "Link": "http://arxiv.org/pdf/2006.05525v7"
    },
    {
      "Year": "2020",
      "Name": "Big Self-Supervised Models Are Strong Semi-Supervised LearnersIF:8Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: We find that, the fewer the labels, the more this approach (task-agnostic use of unlabeled data) benefits from a bigger network.",
      "Link": "http://arxiv.org/pdf/2006.10029v2"
    },
    {
      "Year": "2020",
      "Name": "Reformer: The Efficient TransformerIF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: We introduce two techniques to improve the efficiency of Transformers.",
      "Link": "http://arxiv.org/pdf/2001.04451v2"
    },
    {
      "Year": "2020",
      "Name": "Fourier Neural Operator for Parametric Partial Differential EquationsIF:8Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: In this work, we formulate a new neural operator by parameterizing the integral kernel directly in Fourier space, allowing for an expressive and efficient architecture.",
      "Link": "http://arxiv.org/pdf/2010.08895v3"
    },
    {
      "Year": "2020",
      "Name": "Big Bird: Transformers for Longer SequencesIF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: To remedy this, we propose, BigBird, a sparse attention mechanism that reduces this quadratic dependency to linear.",
      "Link": "http://arxiv.org/pdf/2007.14062v2"
    },
    {
      "Year": "2020",
      "Name": "Offline Reinforcement Learning: Tutorial, Review, And Perspectives On Open ProblemsIF:8Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: In this tutorial article, we aim to provide the reader with the conceptual tools needed to get started on research on offline reinforcement learning algorithms: reinforcement learning algorithms that utilize previously collected data, without additional online data collection.",
      "Link": "http://arxiv.org/pdf/2005.01643v3"
    },
    {
      "Year": "2020",
      "Name": "Meta-Learning In Neural Networks: A SurveyIF:8Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: Contrary to conventional approaches to AI where tasks are solved from scratch using a fixed learning algorithm, meta-learning aims to improve the learning algorithm itself, given the experience of multiple learning episodes.",
      "Link": "http://arxiv.org/pdf/2004.05439v2"
    },
    {
      "Year": "2020",
      "Name": "Graph Contrastive Learning with AugmentationsIF:8Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: In this paper, we propose a graph contrastive learning (GraphCL) framework for learning unsupervised representations of graph data.",
      "Link": "http://arxiv.org/pdf/2010.13902v3"
    },
    {
      "Year": "2020",
      "Name": "Reliable Evaluation Of Adversarial Robustness With An Ensemble Of Diverse Parameter-free AttacksIF:8Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: We apply our ensemble to over 50 models from papers published at recent top machine learning and computer vision venues.",
      "Link": "http://arxiv.org/pdf/2003.01690v2"
    },
    {
      "Year": "2020",
      "Name": "Understanding Contrastive Representation Learning Through Alignment And Uniformity On The HypersphereIF:8Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: In this work, we identify two key properties related to the contrastive loss: (1) alignment (closeness) of features from positive pairs, and (2) uniformity of the induced distribution of the (normalized) features on the hypersphere.",
      "Link": "http://arxiv.org/pdf/2005.10242v10"
    },
    {
      "Year": "2020",
      "Name": "A Review of Uncertainty Quantification in Deep Learning: Techniques, Applications and ChallengesIF:8Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsViewHighlight: In this regard, researchers have proposed different UQ methods and examined their performance in a variety of applications such as computer vision (e.g., self-driving cars and object detection), image processing (e.g., image restoration), medical image analysis (e.g., medical image classification and segmentation), natural language processing (e.g., text classification, social media texts and recidivism risk-scoring), bioinformatics, etc.",
      "Link": "http://arxiv.org/pdf/2011.06225v4"
    },
    {
      "Year": "2020",
      "Name": "On Hyperparameter Optimization of Machine Learning Algorithms: Theory and PracticeIF:8Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: In this paper, optimizing the hyper-parameters of common machine learning models is studied.",
      "Link": "http://arxiv.org/pdf/2007.15745v3"
    },
    {
      "Year": "2020",
      "Name": "Conservative Q-Learning For Offline Reinforcement LearningIF:8Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: In this paper, we propose conservative Q-learning (CQL), which aims to address these limitations by learning a conservative Q-function such that the expected value of a policy under this Q-function lower-bounds its true value.",
      "Link": "http://arxiv.org/pdf/2006.04779v3"
    },
    {
      "Year": "2020",
      "Name": "Linformer: Self-Attention With Linear ComplexityIF:8Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: In this paper, we demonstrate that the self-attention mechanism can be approximated by a low-rank matrix.",
      "Link": "http://arxiv.org/pdf/2006.04768v3"
    },
    {
      "Year": "2020",
      "Name": "Deep Reinforcement Learning for Autonomous Driving: A SurveyIF:8Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsViewHighlight: This review summarises deep reinforcement learning (DRL) algorithms and provides a taxonomy of automated driving tasks where (D)RL methods have been employed, while addressing key computational challenges in real world deployment of autonomous driving agents.",
      "Link": "http://arxiv.org/pdf/2002.00444v2"
    },
    {
      "Year": "2020",
      "Name": "Self-supervised Learning: Generative or ContrastiveIF:8Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsViewHighlight: In this survey, we take a look into new self-supervised learning methods for representation in computer vision, natural language processing, and graph learning.",
      "Link": "http://arxiv.org/pdf/2006.08218v5"
    },
    {
      "Year": "2020",
      "Name": "Rethinking Attention with PerformersIF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: We introduce Performers, Transformer architectures which can estimate regular (softmax) full-rank-attention Transformers with provable accuracy, but using only linear (as opposed to quadratic) space and time complexity, without relying on any priors such as sparsity or low-rankness.",
      "Link": "http://arxiv.org/pdf/2009.14794v4"
    },
    {
      "Year": "2020",
      "Name": "Transformers Are RNNs: Fast Autoregressive Transformers With Linear AttentionIF:8Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: To address this limitation, we express the self-attention as a linear dot-product of kernel feature maps and make use of the associativity property of matrix products to reduce the complexity from $\\mathcal{O}\\left(N^2\\right)$ to $\\mathcal{O}\\left(N\\right)$, where $N$ is the sequence length.",
      "Link": "http://arxiv.org/pdf/2006.16236v3"
    },
    {
      "Year": "2020",
      "Name": "Can AI Help In Screening Viral And COVID-19 Pneumonia?IF:8Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: The aim of this paper is to propose a robust technique for automatic detection of COVID-19 pneumonia from digital chest X-ray images applying pre-trained deep-learning algorithms while maximizing the detection accuracy.",
      "Link": "http://arxiv.org/pdf/2003.13145v3"
    },
    {
      "Year": "2019",
      "Name": "PyTorch: An Imperative Style, High-Performance Deep Learning LibraryIF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: In this paper, we detail the principles that drove the implementation of PyTorch and how they are reflected in its architecture.",
      "Link": "http://arxiv.org/pdf/1912.01703v1"
    },
    {
      "Year": "2019",
      "Name": "Exploring The Limits of Transfer Learning with A Unified Text-to-Text TransformerIF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format.",
      "Link": "http://arxiv.org/pdf/1910.10683v4"
    },
    {
      "Year": "2019",
      "Name": "EfficientNet: Rethinking Model Scaling For Convolutional Neural NetworksIF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance.",
      "Link": "http://arxiv.org/pdf/1905.11946v5"
    },
    {
      "Year": "2019",
      "Name": "A Comprehensive Survey On Graph Neural NetworksIF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: In this survey, we provide a comprehensive overview of graph neural networks (GNNs) in data mining and machine learning fields.",
      "Link": "http://arxiv.org/pdf/1901.00596v4"
    },
    {
      "Year": "2019",
      "Name": "Advances and Open Problems in Federated LearningIF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: Motivated by the explosive growth in FL research, this paper discusses recent advances and presents an extensive collection of open problems and challenges.",
      "Link": "http://arxiv.org/pdf/1912.04977v3"
    },
    {
      "Year": "2019",
      "Name": "NuScenes: A Multimodal Dataset For Autonomous DrivingIF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: In this work we present nuTonomy scenes (nuScenes), the first dataset to carry the full autonomous vehicle sensor suite: 6 cameras, 5 radars and 1 lidar, all with full 360 degree field of view.",
      "Link": "http://arxiv.org/pdf/1903.11027v5"
    },
    {
      "Year": "2019",
      "Name": "Optuna: A Next-generation Hyperparameter Optimization FrameworkIF:8Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: The purpose of this study is to introduce new design-criteria for next-generation hyperparameter optimization software.",
      "Link": "http://arxiv.org/pdf/1907.10902v1"
    },
    {
      "Year": "2019",
      "Name": "Federated Learning: Challenges, Methods, And Future DirectionsIF:8Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: In this article, we discuss the unique characteristics and challenges of federated learning, provide a broad overview of current approaches, and outline several directions of future work that are relevant to a wide range of research communities.",
      "Link": "http://arxiv.org/pdf/1908.07873v1"
    },
    {
      "Year": "2019",
      "Name": "Fast Graph Representation Learning With PyTorch GeometricIF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: In this work, we present the library in detail and perform a comprehensive comparative study of the implemented methods in homogeneous evaluation scenarios.",
      "Link": "http://arxiv.org/pdf/1903.02428v3"
    },
    {
      "Year": "2019",
      "Name": "A Comprehensive Survey On Transfer LearningIF:8Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: Due to the rapid expansion of the transfer learning area, it is both necessary and challenging to comprehensively review the relevant studies.",
      "Link": "http://arxiv.org/pdf/1911.02685v3"
    },
    {
      "Year": "2019",
      "Name": "Transformer-XL: Attentive Language Models Beyond A Fixed-Length ContextIF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: We propose a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence.",
      "Link": "http://arxiv.org/pdf/1901.02860v3"
    },
    {
      "Year": "2019",
      "Name": "A Survey on Bias and Fairness in Machine LearningIF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: In this survey we investigated different real-world applications that have shown biases in various ways, and we listed different sources of biases that can affect AI applications.",
      "Link": "http://arxiv.org/pdf/1908.09635v3"
    },
    {
      "Year": "2019",
      "Name": "Parameter-Efficient Transfer Learning For NLPIF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: As an alternative, we propose transfer with adapter modules.",
      "Link": "http://arxiv.org/pdf/1902.00751v2"
    },
    {
      "Year": "2019",
      "Name": "Benchmarking Neural Network Robustness To Common Corruptions And PerturbationsIF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: In this paper we establish rigorous benchmarks for image classifier robustness. Then we propose a new dataset called ImageNet-P which enables researchers to benchmark a classifier\u2019s robustness to common perturbations.",
      "Link": "http://arxiv.org/pdf/1903.12261v1"
    },
    {
      "Year": "2019",
      "Name": "Generative Modeling By Estimating Gradients Of The Data DistributionIF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: We introduce a new generative model where samples are produced via Langevin dynamics using gradients of the data distribution estimated with score matching.",
      "Link": "http://arxiv.org/pdf/1907.05600v3"
    },
    {
      "Year": "2019",
      "Name": "Simplifying Graph Convolutional NetworksIF:8Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: In this paper, we reduce this excess complexity through successively removing nonlinearities and collapsing weight matrices between consecutive layers.",
      "Link": "http://arxiv.org/pdf/1902.07153v2"
    },
    {
      "Year": "2019",
      "Name": "MixMatch: A Holistic Approach To Semi-Supervised LearningIF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: In this work, we unify the current dominant approaches for semi-supervised learning to produce a new algorithm, MixMatch, that works by guessing low-entropy labels for data-augmented unlabeled examples and mixing labeled and unlabeled data using MixUp.",
      "Link": "http://arxiv.org/pdf/1905.02249v2"
    },
    {
      "Year": "2019",
      "Name": "Towards Federated Learning At Scale: System DesignIF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: In this paper, we describe the resulting high-level design, sketch some of the challenges and their solutions, and touch upon the open problems and future directions.",
      "Link": "http://arxiv.org/pdf/1902.01046v2"
    },
    {
      "Year": "2019",
      "Name": "On The Convergence Of Adam And BeyondIF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: We provide an explicit example of a simple convex optimization setting where Adam does not converge to the optimal solution, and describe the precise problems with the previous analysis of Adam algorithm.",
      "Link": "http://arxiv.org/pdf/1904.09237v1"
    },
    {
      "Year": "2019",
      "Name": "Theoretically Principled Trade-off Between Robustness And AccuracyIF:8Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: In this work, we decompose the prediction error for adversarial examples (robust error) as the sum of the natural (classification) error and boundary error, and provide a differentiable upper bound using the theory of classification-calibrated loss, which is shown to be the tightest possible upper bound uniform over all probability distributions and measurable predictors.",
      "Link": "http://arxiv.org/pdf/1901.08573v3"
    },
    {
      "Year": "2019",
      "Name": "Self-training With Noisy Student Improves ImageNet ClassificationIF:8Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: We present Noisy Student Training, a semi-supervised learning approach that works well even when labeled data is abundant.",
      "Link": "http://arxiv.org/pdf/1911.04252v4"
    },
    {
      "Year": "2019",
      "Name": "Unsupervised Data Augmentation For Consistency TrainingIF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: In this work, we present a new perspective on how to effectively noise unlabeled examples and argue that the quality of noising, specifically those produced by advanced data augmentation methods, plays a crucial role in semi-supervised learning.",
      "Link": "http://arxiv.org/pdf/1904.12848v6"
    },
    {
      "Year": "2019",
      "Name": "SCAFFOLD: Stochastic Controlled Averaging for Federated LearningIF:8Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: As a solution, we propose a new algorithm (SCAFFOLD) which uses control variates (variance reduction) to correct for the `client-drift\u2019 in its local updates.",
      "Link": "http://arxiv.org/pdf/1910.06378v4"
    },
    {
      "Year": "2019",
      "Name": "An Introduction To Variational AutoencodersIF:8Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: In this work, we provide an introduction to variational autoencoders and some important extensions.",
      "Link": "http://arxiv.org/pdf/1906.02691v3"
    },
    {
      "Year": "2019",
      "Name": "RotatE: Knowledge Graph Embedding By Relational Rotation In Complex SpaceIF:8Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: In this paper, we present a new approach for knowledge graph embedding called RotatE, which is able to model and infer various relation patterns including: symmetry/antisymmetry, inversion, and composition.",
      "Link": "http://arxiv.org/pdf/1902.10197v1"
    },
    {
      "Year": "2019",
      "Name": "Certified Adversarial Robustness Via Randomized SmoothingIF:8Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: This randomized smoothing technique has been proposed recently in the literature, but existing guarantees are loose.",
      "Link": "http://arxiv.org/pdf/1902.02918v2"
    },
    {
      "Year": "2019",
      "Name": "Deep Leakage From GradientsIF:8Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewAbstract:Exchanging gradients is a widely used method in modern multi-node machine learning system (e.g., distributed training, collaborative learning). For a long time, people believed \u2026",
      "Link": "http://arxiv.org/pdf/1906.08935v2"
    },
    {
      "Year": "2019",
      "Name": "Introduction to Online Convex OptimizationIF:8Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: It is necessary as well as beneficial to take a robust approach, by applying an optimization method that learns as one goes along, learning from experience as more aspects of the problem are observed.",
      "Link": "http://arxiv.org/pdf/1909.05207v3"
    },
    {
      "Year": "2019",
      "Name": "Mastering Atari, Go, Chess And Shogi By Planning With A Learned ModelIF:8Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: In this work we present the MuZero algorithm which, by combining a tree-based search with a learned model, achieves superhuman performance in a range of challenging and visually complex domains, without any knowledge of their underlying dynamics.",
      "Link": "http://arxiv.org/pdf/1911.08265v2"
    },
    {
      "Year": "2019",
      "Name": "On The Variance of The Adaptive Learning Rate and BeyondIF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: Pursuing the theory behind warmup, we identify a problem of the adaptive learning rate (i.e., it has problematically large variance in the early stage), suggest warmup works as a variance reduction technique, and provide both empirical and theoretical evidence to verify our hypothesis.",
      "Link": "http://arxiv.org/pdf/1908.03265v4"
    },
    {
      "Year": "2018",
      "Name": "Representation Learning With Contrastive Predictive CodingIF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: In this work, we propose a universal unsupervised learning approach to extract useful representations from high-dimensional data, which we call Contrastive Predictive Coding.",
      "Link": "http://arxiv.org/pdf/1807.03748v2"
    },
    {
      "Year": "2018",
      "Name": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning With A Stochastic ActorIF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: In this paper, we propose soft actor-critic, an off-policy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework.",
      "Link": "http://arxiv.org/pdf/1801.01290v2"
    },
    {
      "Year": "2018",
      "Name": "How Powerful Are Graph Neural Networks?IF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: Here, we present a theoretical framework for analyzing the expressive power of GNNs to capture different graph structures.",
      "Link": "http://arxiv.org/pdf/1810.00826v3"
    },
    {
      "Year": "2018",
      "Name": "Large Scale GAN Training For High Fidelity Natural Image SynthesisIF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: To this end, we train Generative Adversarial Networks at the largest scale yet attempted, and study the instabilities specific to such scale.",
      "Link": "http://arxiv.org/pdf/1809.11096v2"
    },
    {
      "Year": "2018",
      "Name": "Graph Neural Networks: A Review of Methods and ApplicationsIF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: In this survey, we propose a general design pipeline for GNN models and discuss the variants of each component, systematically categorize the applications, and propose four open problems for future research.",
      "Link": "http://arxiv.org/pdf/1812.08434v6"
    },
    {
      "Year": "2018",
      "Name": "Neural Ordinary Differential EquationsIF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: We introduce a new family of deep neural network models.",
      "Link": "http://arxiv.org/pdf/1806.07366v5"
    },
    {
      "Year": "2018",
      "Name": "Spectral Normalization For Generative Adversarial NetworksIF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: In this paper, we propose a novel weight normalization technique called spectral normalization to stabilize the training of the discriminator.",
      "Link": "http://arxiv.org/pdf/1802.05957v1"
    },
    {
      "Year": "2018",
      "Name": "An Empirical Evaluation Of Generic Convolutional And Recurrent Networks For Sequence ModelingIF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: To assist related work, we have made code available at http://github.com/locuslab/TCN .",
      "Link": "http://arxiv.org/pdf/1803.01271v2"
    },
    {
      "Year": "2018",
      "Name": "DARTS: Differentiable Architecture SearchIF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: This paper addresses the scalability challenge of architecture search by formulating the task in a differentiable manner.",
      "Link": "http://arxiv.org/pdf/1806.09055v2"
    },
    {
      "Year": "2018",
      "Name": "Federated Optimization In Heterogeneous NetworksIF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: In this work, we introduce a framework, FedProx, to tackle heterogeneity in federated networks.",
      "Link": "http://arxiv.org/pdf/1812.06127v5"
    },
    {
      "Year": "2018",
      "Name": "The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural NetworksIF:8Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: We present an algorithm to identify winning tickets and a series of experiments that support the lottery ticket hypothesis and the importance of these fortuitous initializations.",
      "Link": "http://arxiv.org/pdf/1803.03635v5"
    },
    {
      "Year": "2018",
      "Name": "Obfuscated Gradients Give A False Sense Of Security: Circumventing Defenses To Adversarial ExamplesIF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: We describe characteristic behaviors of defenses exhibiting the effect, and for each of the three types of obfuscated gradients we discover, we develop attack techniques to overcome it.",
      "Link": "http://arxiv.org/pdf/1802.00420v4"
    },
    {
      "Year": "2018",
      "Name": "PointPillars: Fast Encoders For Object Detection From Point CloudsIF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: In this paper we consider the problem of encoding a point cloud into a format appropriate for a downstream detection pipeline.",
      "Link": "http://arxiv.org/pdf/1812.05784v2"
    },
    {
      "Year": "2018",
      "Name": "Relational Inductive Biases, Deep Learning, And Graph NetworksIF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: We present a new building block for the AI toolkit with a strong relational inductive bias\u2013the graph network\u2013which generalizes and extends various approaches for neural networks that operate on graphs, and provides a straightforward interface for manipulating structured knowledge and producing structured behaviors.",
      "Link": "http://arxiv.org/pdf/1806.01261v3"
    },
    {
      "Year": "2018",
      "Name": "Fundamentals of Recurrent Neural Network (RNN) and Long Short-Term Memory (LSTM) NetworkIF:8Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: The goal of this paper is to explain the essential RNN and LSTM fundamentals in a single document.",
      "Link": "http://arxiv.org/pdf/1808.03314v10"
    },
    {
      "Year": "2018",
      "Name": "Neural Tangent Kernel: Convergence And Generalization In Neural NetworksIF:8Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: We prove the positive-definiteness of the limiting NTK when the data is supported on the sphere and the non-linearity is non-polynomial.",
      "Link": "http://arxiv.org/pdf/1806.07572v4"
    },
    {
      "Year": "2018",
      "Name": "Efficient Neural Architecture Search Via Parameter SharingIF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: We propose Efficient Neural Architecture Search (ENAS), a fast and inexpensive approach for automatic model design.",
      "Link": "http://arxiv.org/pdf/1802.03268v2"
    },
    {
      "Year": "2018",
      "Name": "Continual Lifelong Learning With Neural Networks: A ReviewIF:8Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsViewHighlight: In this review, we critically summarize the main challenges linked to lifelong learning for artificial learning systems and compare existing neural network approaches that alleviate, to different extents, catastrophic forgetting.",
      "Link": "http://arxiv.org/pdf/1802.07569v4"
    },
    {
      "Year": "2018",
      "Name": "Deep Learning In Agriculture: A SurveyIF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsViewHighlight: In this paper, we perform a survey of 40 research efforts that employ deep learning techniques, applied to various agricultural and food production challenges.",
      "Link": "http://arxiv.org/pdf/1807.11809v1"
    },
    {
      "Year": "2018",
      "Name": "Deeper Insights Into Graph Convolutional Networks For Semi-Supervised LearningIF:8Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: In this paper, we develop deeper insights into the GCN model and address its fundamental limits.",
      "Link": "http://arxiv.org/pdf/1801.07606v1"
    },
    {
      "Year": "2018",
      "Name": "A Survey On Deep Transfer LearningIF:8Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsViewHighlight: This survey focuses on reviewing the current researches of transfer learning by using deep neural network and its applications.",
      "Link": "http://arxiv.org/pdf/1808.01974v1"
    },
    {
      "Year": "2018",
      "Name": "Deep Learning For Time Series Classification: A ReviewIF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: In this article, we study the current state-of-the-art performance of deep learning algorithms for TSC by presenting an empirical study of the most recent DNN architectures for TSC.",
      "Link": "http://arxiv.org/pdf/1809.04356v4"
    },
    {
      "Year": "2018",
      "Name": "Generalized Cross Entropy Loss For Training Deep Neural Networks With Noisy LabelsIF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: Here, we present a theoretically grounded set of noise-robust loss functions that can be seen as a generalization of MAE and CCE.",
      "Link": "http://arxiv.org/pdf/1805.07836v4"
    },
    {
      "Year": "2018",
      "Name": "Federated Learning with Non-IID DataIF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: In this work, we focus on the statistical challenge of federated learning when local data is non-IID.",
      "Link": "http://arxiv.org/pdf/1806.00582v2"
    },
    {
      "Year": "2018",
      "Name": "On First-Order Meta-Learning AlgorithmsIF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: It also includes Reptile, a new algorithm that we introduce here, which works by repeatedly sampling a task, training on it, and moving the initialization towards the trained weights on that task.",
      "Link": "http://arxiv.org/pdf/1803.02999v3"
    },
    {
      "Year": "2018",
      "Name": "Soft Actor-Critic Algorithms And ApplicationsIF:8Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: In this paper, we describe Soft Actor-Critic (SAC), our recently introduced off-policy actor-critic algorithm based on the maximum entropy RL framework.",
      "Link": "http://arxiv.org/pdf/1812.05905v2"
    },
    {
      "Year": "2018",
      "Name": "Hierarchical Graph Representation Learning With Differentiable PoolingIF:8Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: Here we propose DiffPool, a differentiable graph pooling module that can generate hierarchical representations of graphs and can be combined with various graph neural network architectures in an end-to-end fashion.",
      "Link": "http://arxiv.org/pdf/1806.08804v4"
    },
    {
      "Year": "2018",
      "Name": "Co-teaching: Robust Training Of Deep Neural Networks With Extremely Noisy LabelsIF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: Therefore in this paper, we propose a new deep learning paradigm called Co-teaching for combating with noisy labels.",
      "Link": "http://arxiv.org/pdf/1804.06872v3"
    },
    {
      "Year": "2018",
      "Name": "ProxylessNAS: Direct Neural Architecture Search On Target Task And HardwareIF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: In this paper, we present \\emph{ProxylessNAS} that can \\emph{directly} learn the architectures for large-scale target tasks and target hardware platforms.",
      "Link": "http://arxiv.org/pdf/1812.00332v2"
    },
    {
      "Year": "2018",
      "Name": "Representation Learning On Graphs With Jumping Knowledge NetworksIF:8Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: We analyze some important properties of these models, and propose a strategy to overcome those.",
      "Link": "http://arxiv.org/pdf/1806.03536v2"
    },
    {
      "Year": "2017",
      "Name": "Decoupled Weight Decay RegularizationIF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: L$_2$ regularization and weight decay regularization are equivalent for standard stochastic gradient descent (when rescaled by the learning rate), but as we demonstrate this is \\emph{not} the case for adaptive gradient algorithms, such as Adam.",
      "Link": "http://arxiv.org/pdf/1711.05101v3"
    },
    {
      "Year": "2017",
      "Name": "Proximal Policy Optimization AlgorithmsIF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a surrogate objective function using stochastic gradient ascent.",
      "Link": "http://arxiv.org/pdf/1707.06347v2"
    },
    {
      "Year": "2017",
      "Name": "GANs Trained By A Two Time-Scale Update Rule Converge To A Local Nash EquilibriumIF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: We propose a two time-scale update rule (TTUR) for training GANs with stochastic gradient descent on arbitrary GAN loss functions.",
      "Link": "http://arxiv.org/pdf/1706.08500v6"
    },
    {
      "Year": "2017",
      "Name": "Model-Agnostic Meta-Learning For Fast Adaptation Of Deep NetworksIF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: We propose an algorithm for meta-learning that is model-agnostic, in the sense that it is compatible with any model trained with gradient descent and applicable to a variety of different learning problems, including classification, regression, and reinforcement learning.",
      "Link": "http://arxiv.org/pdf/1703.03400v3"
    },
    {
      "Year": "2017",
      "Name": "Improved Training Of Wasserstein GANsIF:10Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: We propose an alternative to clipping weights: penalize the norm of gradient of the critic with respect to its input.",
      "Link": "http://arxiv.org/pdf/1704.00028v3"
    },
    {
      "Year": "2017",
      "Name": "Mixup: Beyond Empirical Risk MinimizationIF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: In this work, we propose mixup, a simple learning principle to alleviate these issues.",
      "Link": "http://arxiv.org/pdf/1710.09412v2"
    },
    {
      "Year": "2017",
      "Name": "Fashion-MNIST: A Novel Image Dataset For Benchmarking Machine Learning AlgorithmsIF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: We present Fashion-MNIST, a new dataset comprising of 28\u00d728 grayscale images of 70,000 fashion products from 10 categories, with 7,000 images per category.",
      "Link": "http://arxiv.org/pdf/1708.07747v2"
    },
    {
      "Year": "2017",
      "Name": "Prototypical Networks For Few-shot LearningIF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: We propose prototypical networks for the problem of few-shot classification, where a classifier must generalize to new classes not seen in the training set, given only a small number of examples of each new class.",
      "Link": "http://arxiv.org/pdf/1703.05175v2"
    },
    {
      "Year": "2017",
      "Name": "Neural Message Passing For Quantum ChemistryIF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: In this paper, we reformulate existing models into a single common framework we call Message Passing Neural Networks (MPNNs) and explore additional novel variations within this framework.",
      "Link": "http://arxiv.org/pdf/1704.01212v2"
    },
    {
      "Year": "2017",
      "Name": "Axiomatic Attribution For Deep NetworksIF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: We study the problem of attributing the prediction of a deep network to its input features, a problem previously studied by several other works.",
      "Link": "http://arxiv.org/pdf/1703.01365v2"
    },
    {
      "Year": "2017",
      "Name": "On Calibration Of Modern Neural NetworksIF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: We discover that modern neural networks, unlike those from a decade ago, are poorly calibrated.",
      "Link": "http://arxiv.org/pdf/1706.04599v2"
    },
    {
      "Year": "2017",
      "Name": "CARLA: An Open Urban Driving SimulatorIF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: We introduce CARLA, an open-source simulator for autonomous driving research.",
      "Link": "http://arxiv.org/pdf/1711.03938v1"
    },
    {
      "Year": "2017",
      "Name": "Multi-Agent Actor-Critic For Mixed Cooperative-Competitive EnvironmentsIF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: We explore deep reinforcement learning methods for multi-agent domains.",
      "Link": "http://arxiv.org/pdf/1706.02275v4"
    },
    {
      "Year": "2017",
      "Name": "Neural Discrete Representation LearningIF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: In this paper, we propose a simple yet powerful generative model that learns such discrete representations.",
      "Link": "http://arxiv.org/pdf/1711.00937v2"
    },
    {
      "Year": "2017",
      "Name": "Quantization And Training Of Neural Networks For Efficient Integer-Arithmetic-Only InferenceIF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: We propose a quantization scheme that allows inference to be carried out using integer-only arithmetic, which can be implemented more efficiently than floating point inference on commonly available integer-only hardware.",
      "Link": "http://arxiv.org/pdf/1712.05877v1"
    },
    {
      "Year": "2017",
      "Name": "An Overview Of Multi-Task Learning In Deep Neural NetworksIF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: This article aims to give a general overview of MTL, particularly in deep neural networks.",
      "Link": "http://arxiv.org/pdf/1706.05098v1"
    },
    {
      "Year": "2017",
      "Name": "Diffusion Convolutional Recurrent Neural Network: Data-Driven Traffic ForecastingIF:8Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: To address these challenges, we propose to model the traffic flow as a diffusion process on a directed graph and introduce Diffusion Convolutional Recurrent Neural Network (DCRNN), a deep learning framework for traffic forecasting that incorporates both spatial and temporal dependency in the traffic flow.",
      "Link": "http://arxiv.org/pdf/1707.01926v3"
    },
    {
      "Year": "2017",
      "Name": "Multimodal Machine Learning: A Survey And TaxonomyIF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsViewHighlight: Instead of focusing on specific multimodal applications, this paper surveys the recent advances in multimodal machine learning itself and presents them in a common taxonomy.",
      "Link": "http://arxiv.org/pdf/1705.09406v2"
    },
    {
      "Year": "2017",
      "Name": "Boosting Adversarial Attacks With MomentumIF:8Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: To address this issue, we propose a broad class of momentum-based iterative algorithms to boost adversarial attacks.",
      "Link": "http://arxiv.org/pdf/1710.06081v3"
    },
    {
      "Year": "2017",
      "Name": "Gradient Episodic Memory for Continual LearningIF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: One major obstacle towards AI is the poor ability of models to solve new problems quicker, and without forgetting previously acquired knowledge. To better understand this issue, we study the problem of continual learning, where the model observes, once and one by one, examples concerning a sequence of tasks.",
      "Link": "http://arxiv.org/pdf/1706.08840v6"
    },
    {
      "Year": "2017",
      "Name": "Continual Learning Through Synaptic IntelligenceIF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: In this study, we introduce intelligent synapses that bring some of this biological complexity into artificial neural networks.",
      "Link": "http://arxiv.org/pdf/1703.04200v3"
    },
    {
      "Year": "2017",
      "Name": "Convolutional 2D Knowledge Graph EmbeddingsIF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: In this work, we introduce ConvE, a multi-layer convolutional network model for link prediction, and report state-of-the-art results for several established datasets.",
      "Link": "http://arxiv.org/pdf/1707.01476v6"
    },
    {
      "Year": "2017",
      "Name": "Deep SetsIF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: We study the problem of designing models for machine learning tasks defined on \\emph{sets}.",
      "Link": "http://arxiv.org/pdf/1703.06114v3"
    },
    {
      "Year": "2017",
      "Name": "Curiosity-driven Exploration By Self-supervised PredictionIF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: We formulate curiosity as the error in an agent\u2019s ability to predict the consequence of its own actions in a visual feature space learned by a self-supervised inverse dynamics model.",
      "Link": "http://arxiv.org/pdf/1705.05363v1"
    },
    {
      "Year": "2017",
      "Name": "One Pixel Attack For Fooling Deep Neural NetworksIF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: In this paper, we analyze an attack in an extremely limited scenario where only one pixel can be modified.",
      "Link": "http://arxiv.org/pdf/1710.08864v7"
    },
    {
      "Year": "2017",
      "Name": "Hindsight Experience ReplayIF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: We present a novel technique called Hindsight Experience Replay which allows sample-efficient learning from rewards which are sparse and binary and therefore avoid the need for complicated reward engineering.",
      "Link": "http://arxiv.org/pdf/1707.01495v3"
    },
    {
      "Year": "2017",
      "Name": "Methods For Interpreting And Understanding Deep Neural NetworksIF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsViewHighlight: It introduces some recently proposed techniques of interpretation, along with theory, tricks and recommendations, to make most efficient use of these techniques on real data.",
      "Link": "http://arxiv.org/pdf/1706.07979v1"
    },
    {
      "Year": "2017",
      "Name": "Conditional Adversarial Domain AdaptationIF:8Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: In this paper, we present conditional adversarial domain adaptation, a principled framework that conditions the adversarial adaptation models on discriminative information conveyed in the classifier predictions.",
      "Link": "http://arxiv.org/pdf/1705.10667v4"
    },
    {
      "Year": "2017",
      "Name": "SmoothGrad: Removing Noise By Adding NoiseIF:8Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: This paper makes two contributions: it introduces SmoothGrad, a simple method that can help visually sharpen gradient-based sensitivity maps, and it discusses lessons in the visualization of these maps.",
      "Link": "http://arxiv.org/pdf/1706.03825v1"
    },
    {
      "Year": "2017",
      "Name": "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts LayerIF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: In this work, we address these challenges and finally realize the promise of conditional computation, achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters.",
      "Link": "http://arxiv.org/pdf/1701.06538v1"
    },
    {
      "Year": "2016",
      "Name": "XGBoost: A Scalable Tree Boosting SystemIF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: We propose a novel sparsity-aware algorithm for sparse data and weighted quantile sketch for approximate tree learning.",
      "Link": "http://arxiv.org/pdf/1603.02754v3"
    },
    {
      "Year": "2016",
      "Name": "Semi-Supervised Classification With Graph Convolutional NetworksIF:10Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs.",
      "Link": "http://arxiv.org/pdf/1609.02907v4"
    },
    {
      "Year": "2016",
      "Name": "Communication-Efficient Learning of Deep Networks from Decentralized DataIF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: We present a practical method for the federated learning of deep networks based on iterative model averaging, and conduct an extensive empirical evaluation, considering five different model architectures and four datasets.",
      "Link": "http://arxiv.org/pdf/1602.05629v4"
    },
    {
      "Year": "2016",
      "Name": "Why Should I Trust You?: Explaining The Predictions Of Any ClassifierIF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsViewHighlight: In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction.",
      "Link": "http://arxiv.org/pdf/1602.04938v3"
    },
    {
      "Year": "2016",
      "Name": "Asynchronous Methods For Deep Reinforcement LearningIF:10Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers.",
      "Link": "http://arxiv.org/pdf/1602.01783v2"
    },
    {
      "Year": "2016",
      "Name": "Convolutional Neural Networks On Graphs With Fast Localized Spectral FilteringIF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: In this work, we are interested in generalizing convolutional neural networks (CNNs) from low-dimensional regular grids, where image, video and speech are represented, to high-dimensional irregular domains, such as social networks, brain connectomes or words\u2019 embedding, represented by graphs.",
      "Link": "http://arxiv.org/pdf/1606.09375v3"
    },
    {
      "Year": "2016",
      "Name": "Matching Networks For One Shot LearningIF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: In this work, we employ ideas from metric learning based on deep neural features and from recent advances that augment neural networks with external memories.",
      "Link": "http://arxiv.org/pdf/1606.04080v2"
    },
    {
      "Year": "2016",
      "Name": "Overcoming Catastrophic Forgetting In Neural NetworksIF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: We demonstrate our approach is scalable and effective by solving a set of classification tasks based on the MNIST hand written digit dataset and by learning several Atari 2600 games sequentially.",
      "Link": "http://arxiv.org/pdf/1612.00796v2"
    },
    {
      "Year": "2016",
      "Name": "An Overview Of Gradient Descent Optimization AlgorithmsIF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: In the course of this overview, we look at different variants of gradient descent, summarize challenges, introduce the most common optimization algorithms, review architectures in a parallel and distributed setting, and investigate additional strategies for optimizing gradient descent.",
      "Link": "http://arxiv.org/pdf/1609.04747v2"
    },
    {
      "Year": "2016",
      "Name": "Neural Architecture Search With Reinforcement LearningIF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: In this paper, we use a recurrent network to generate the model descriptions of neural networks and train this RNN with reinforcement learning to maximize the expected accuracy of the generated architectures on a validation set.",
      "Link": "http://arxiv.org/pdf/1611.01578v2"
    },
    {
      "Year": "2016",
      "Name": "OpenAI GymIF:9SummaryRelated PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewAbstract:OpenAI Gym is a toolkit for reinforcement learning research. It includes a growing collection of benchmark problems that expose a common interface, and a website where people can \u2026",
      "Link": "http://arxiv.org/pdf/1606.01540v1"
    },
    {
      "Year": "2016",
      "Name": "Understanding Deep Learning Requires Rethinking GeneralizationIF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: Through extensive systematic experiments, we show how these traditional approaches fail to explain why large neural networks generalize well in practice.",
      "Link": "http://arxiv.org/pdf/1611.03530v2"
    },
    {
      "Year": "2016",
      "Name": "Federated Learning: Strategies For Improving Communication EfficiencyIF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsViewHighlight: In this paper, we propose two ways to reduce the uplink communication costs: structured updates, where we directly learn an update from a restricted space parametrized using a smaller number of variables, e.g. either low-rank or a random mask; and sketched updates, where we learn a full model update and then compress it using a combination of quantization, random rotations, and subsampling before sending it to the server.",
      "Link": "http://arxiv.org/pdf/1610.05492v2"
    },
    {
      "Year": "2016",
      "Name": "InfoGAN: Interpretable Representation Learning By Information Maximizing Generative Adversarial NetsIF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: This paper describes InfoGAN, an information-theoretic extension to the Generative Adversarial Network that is able to learn disentangled representations in a completely unsupervised manner.",
      "Link": "http://arxiv.org/pdf/1606.03657v1"
    },
    {
      "Year": "2016",
      "Name": "Gaussian Error Linear Units (GELUs)IF:8Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: We propose the Gaussian Error Linear Unit (GELU), a high-performing neural network activation function.",
      "Link": "http://arxiv.org/pdf/1606.08415v5"
    },
    {
      "Year": "2016",
      "Name": "Equality Of Opportunity In Supervised LearningIF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: We propose a criterion for discrimination against a specified sensitive attribute in supervised learning, where the goal is to predict some target based on available features.",
      "Link": "http://arxiv.org/pdf/1610.02413v1"
    },
    {
      "Year": "2016",
      "Name": "Wide & Deep Learning For Recommender SystemsIF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: In this paper, we present Wide & Deep learning\u2014jointly trained wide linear models and deep neural networks\u2014to combine the benefits of memorization and generalization for recommender systems.",
      "Link": "http://arxiv.org/pdf/1606.07792v1"
    },
    {
      "Year": "2016",
      "Name": "Generative Adversarial Imitation LearningIF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: We propose a new general framework for directly extracting a policy from data, as if it were obtained by reinforcement learning following inverse reinforcement learning.",
      "Link": "http://arxiv.org/pdf/1606.03476v1"
    },
    {
      "Year": "2016",
      "Name": "On Large-Batch Training For Deep Learning: Generalization Gap And Sharp MinimaIF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: We investigate the cause for this generalization drop in the large-batch regime and present numerical evidence that supports the view that large-batch methods tend to converge to sharp minimizers of the training and testing functions \u2013 and as is well known, sharp minima lead to poorer generalization.",
      "Link": "http://arxiv.org/pdf/1609.04836v2"
    },
    {
      "Year": "2016",
      "Name": "Automatic Chemical Design Using A Data-driven Continuous Representation Of MoleculesIF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: We report a method to convert discrete representations of molecules to and from a multidimensional continuous representation.",
      "Link": "http://arxiv.org/pdf/1610.02415v3"
    },
    {
      "Year": "2016",
      "Name": "End-to-end Sequence Labeling Via Bi-directional LSTM-CNNs-CRFIF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: In this paper, we introduce a novel neutral network architecture that benefits from both word- and character-level representations automatically, by using combination of bidirectional LSTM, CNN and CRF.",
      "Link": "http://arxiv.org/pdf/1603.01354v5"
    },
    {
      "Year": "2016",
      "Name": "The Concrete Distribution: A Continuous Relaxation Of Discrete Random VariablesIF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: In this work we introduce Concrete random variables\u2014continuous relaxations of discrete random variables.",
      "Link": "http://arxiv.org/pdf/1611.00712v3"
    },
    {
      "Year": "2016",
      "Name": "SeqGAN: Sequence Generative Adversarial Nets With Policy GradientIF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: In this paper, we propose a sequence generation framework, called SeqGAN, to solve the problems.",
      "Link": "http://arxiv.org/pdf/1609.05473v6"
    },
    {
      "Year": "2016",
      "Name": "Deep Transfer Learning With Joint Adaptation NetworksIF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: In this paper, we present joint adaptation networks (JAN), which learn a transfer network by aligning the joint distributions of multiple domain-specific layers across domains based on a joint maximum mean discrepancy (JMMD) criterion.",
      "Link": "http://arxiv.org/pdf/1605.06636v2"
    },
    {
      "Year": "2016",
      "Name": "Progressive Neural NetworksIF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: Using a novel sensitivity measure, we demonstrate that transfer occurs at both low-level sensory and high-level control layers of the learned policy.",
      "Link": "http://arxiv.org/pdf/1606.04671v4"
    },
    {
      "Year": "2016",
      "Name": "Deep Networks With Stochastic DepthIF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: To address these problems, we propose stochastic depth, a training procedure that enables the seemingly contradictory setup to train short networks and use deep networks at test time.",
      "Link": "http://arxiv.org/pdf/1603.09382v3"
    },
    {
      "Year": "2016",
      "Name": "Binarized Neural Networks: Training Deep Neural Networks With Weights And Activations Constrained To +1 Or -1IF:8Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: We introduce a method to train Binarized Neural Networks (BNNs) \u2013 neural networks with binary weights and activations at run-time.",
      "Link": "http://arxiv.org/pdf/1602.02830v3"
    },
    {
      "Year": "2016",
      "Name": "Hyperband: A Novel Bandit-Based Approach To Hyperparameter OptimizationIF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: We introduce a novel algorithm, Hyperband, for this framework and analyze its theoretical properties, providing several desirable guarantees.",
      "Link": "http://arxiv.org/pdf/1603.06560v4"
    },
    {
      "Year": "2016",
      "Name": "Learning Hand-Eye Coordination For Robotic Grasping With Deep Learning And Large-Scale Data CollectionIF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsViewHighlight: We describe a learning-based approach to hand-eye coordination for robotic grasping from monocular images.",
      "Link": "http://arxiv.org/pdf/1603.02199v4"
    },
    {
      "Year": "2016",
      "Name": "Revisiting Semi-Supervised Learning With Graph EmbeddingsIF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: We present a semi-supervised learning framework based on graph embeddings.",
      "Link": "http://arxiv.org/pdf/1603.08861v2"
    },
    {
      "Year": "2015",
      "Name": "Batch Normalization: Accelerating Deep Network Training By Reducing Internal Covariate ShiftIF:10Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs.",
      "Link": "http://arxiv.org/pdf/1502.03167v3"
    },
    {
      "Year": "2015",
      "Name": "Unsupervised Representation Learning With Deep Convolutional Generative Adversarial NetworksIF:10Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning.",
      "Link": "http://arxiv.org/pdf/1511.06434v2"
    },
    {
      "Year": "2015",
      "Name": "Continuous Control With Deep Reinforcement LearningIF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces.",
      "Link": "http://arxiv.org/pdf/1509.02971v6"
    },
    {
      "Year": "2015",
      "Name": "Show, Attend And Tell: Neural Image Caption Generation With Visual AttentionIF:10Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: Inspired by recent work in machine translation and object detection, we introduce an attention based model that automatically learns to describe the content of images.",
      "Link": "http://arxiv.org/pdf/1502.03044v3"
    },
    {
      "Year": "2015",
      "Name": "Deep Reinforcement Learning With Double Q-learningIF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: In this paper, we answer all these questions affirmatively.",
      "Link": "http://arxiv.org/pdf/1509.06461v3"
    },
    {
      "Year": "2015",
      "Name": "Trust Region Policy OptimizationIF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: We describe an iterative procedure for optimizing policies, with guaranteed monotonic improvement.",
      "Link": "http://arxiv.org/pdf/1502.05477v5"
    },
    {
      "Year": "2015",
      "Name": "Character-level Convolutional Networks For Text ClassificationIF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: This article offers an empirical exploration on the use of character-level convolutional networks (ConvNets) for text classification.",
      "Link": "http://arxiv.org/pdf/1509.01626v3"
    },
    {
      "Year": "2015",
      "Name": "Fast And Accurate Deep Network Learning By Exponential Linear Units (ELUs)IF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: We introduce the exponential linear unit (ELU) which speeds up learning in deep neural networks and leads to higher classification accuracies.",
      "Link": "http://arxiv.org/pdf/1511.07289v5"
    },
    {
      "Year": "2015",
      "Name": "LINE: Large-scale Information Network EmbeddingIF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: In this paper, we propose a novel network embedding method called the LINE, which is suitable for arbitrary types of information networks: undirected, directed, and/or weighted.",
      "Link": "http://arxiv.org/pdf/1503.03578v1"
    },
    {
      "Year": "2015",
      "Name": "Deep Unsupervised Learning Using Nonequilibrium ThermodynamicsIF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: Here, we develop an approach that simultaneously achieves both flexibility and tractability.",
      "Link": "http://arxiv.org/pdf/1503.03585v8"
    },
    {
      "Year": "2015",
      "Name": "Learning Transferable Features With Deep Adaptation NetworksIF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: In this paper, we propose a new Deep Adaptation Network (DAN) architecture, which generalizes deep convolutional neural network to the domain adaptation scenario.",
      "Link": "http://arxiv.org/pdf/1502.02791v2"
    },
    {
      "Year": "2015",
      "Name": "DeepFool: A Simple And Accurate Method To Fool Deep Neural NetworksIF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: In this paper, we fill this gap and propose the DeepFool algorithm to efficiently compute perturbations that fool deep networks, and thus reliably quantify the robustness of these classifiers.",
      "Link": "http://arxiv.org/pdf/1511.04599v3"
    },
    {
      "Year": "2015",
      "Name": "Prioritized Experience ReplayIF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: In this paper we develop a framework for prioritizing experience, so as to replay important transitions more frequently, and therefore learn more efficiently.",
      "Link": "http://arxiv.org/pdf/1511.05952v4"
    },
    {
      "Year": "2015",
      "Name": "Dueling Network Architectures For Deep Reinforcement LearningIF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: In this paper, we present a new neural network architecture for model-free reinforcement learning.",
      "Link": "http://arxiv.org/pdf/1511.06581v3"
    },
    {
      "Year": "2015",
      "Name": "End-to-End Training Of Deep Visuomotor PoliciesIF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsViewHighlight: In this paper, we aim to answer the following question: does training the perception and control systems jointly end-to-end provide better performance than training each component separately?",
      "Link": "http://arxiv.org/pdf/1504.00702v5"
    },
    {
      "Year": "2015",
      "Name": "Convolutional Networks On Graphs For Learning Molecular FingerprintsIF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: We introduce a convolutional neural network that operates directly on graphs.",
      "Link": "http://arxiv.org/pdf/1509.09292v2"
    },
    {
      "Year": "2015",
      "Name": "Gated Graph Sequence Neural NetworksIF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: In this work, we study feature learning techniques for graph-structured inputs.",
      "Link": "http://arxiv.org/pdf/1511.05493v4"
    },
    {
      "Year": "2015",
      "Name": "High-Dimensional Continuous Control Using Generalized Advantage EstimationIF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: We address the first challenge by using value functions to substantially reduce the variance of policy gradient estimates at the cost of some bias, with an exponentially-weighted estimator of the advantage function that is analogous to TD(lambda).",
      "Link": "http://arxiv.org/pdf/1506.02438v6"
    },
    {
      "Year": "2015",
      "Name": "BinaryConnect: Training Deep Neural Networks With Binary Weights During PropagationsIF:10Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: We introduce BinaryConnect, a method which consists in training a DNN with binary weights during the forward and backward propagations, while retaining precision of the stored weights in which gradients are accumulated.",
      "Link": "http://arxiv.org/pdf/1511.00363v3"
    },
    {
      "Year": "2015",
      "Name": "Empirical Evaluation Of Rectified Activations In Convolutional NetworkIF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: In this paper we investigate the performance of different types of rectified activation functions in convolutional neural network: standard rectified linear unit (ReLU), leaky rectified linear unit (Leaky ReLU), parametric rectified linear unit (PReLU) and a new randomized leaky rectified linear units (RReLU).",
      "Link": "http://arxiv.org/pdf/1505.00853v2"
    },
    {
      "Year": "2015",
      "Name": "Unsupervised Deep Embedding For Clustering AnalysisIF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: In this paper, we propose Deep Embedded Clustering (DEC), a method that simultaneously learns feature representations and cluster assignments using deep neural networks.",
      "Link": "http://arxiv.org/pdf/1511.06335v2"
    },
    {
      "Year": "2015",
      "Name": "Unsupervised Learning Of Video Representations Using LSTMsIF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: We use multilayer Long Short Term Memory (LSTM) networks to learn representations of video sequences.",
      "Link": "http://arxiv.org/pdf/1502.04681v3"
    },
    {
      "Year": "2015",
      "Name": "Generating Sentences From A Continuous SpaceIF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: In this work, we introduce and study an RNN-based variational autoencoder generative model that incorporates distributed latent representations of entire sentences.",
      "Link": "http://arxiv.org/pdf/1511.06349v4"
    },
    {
      "Year": "2015",
      "Name": "A Critical Review Of Recurrent Neural Networks For Sequence LearningIF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: In this survey, we review and synthesize the research that over the past three decades first yielded and then made practical these powerful learning models.",
      "Link": "http://arxiv.org/pdf/1506.00019v4"
    },
    {
      "Year": "2015",
      "Name": "Adversarial AutoencodersIF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: In this paper, we propose the adversarial autoencoder (AAE), which is a probabilistic autoencoder that uses the recently proposed generative adversarial networks (GAN) to perform variational inference by matching the aggregated posterior of the hidden code vector of the autoencoder with an arbitrary prior distribution.",
      "Link": "http://arxiv.org/pdf/1511.05644v2"
    },
    {
      "Year": "2015",
      "Name": "Deep Learning With Limited Numerical PrecisionIF:10Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: We study the effect of limited precision data representation and computation on neural network training.",
      "Link": "http://arxiv.org/pdf/1502.02551v1"
    },
    {
      "Year": "2015",
      "Name": "Scheduled Sampling For Sequence Prediction With Recurrent Neural NetworksIF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: We propose a curriculum learning strategy to gently change the training process from a fully guided scheme using the true previous token, towards a less guided scheme which mostly uses the generated token instead.",
      "Link": "http://arxiv.org/pdf/1506.03099v3"
    },
    {
      "Year": "2015",
      "Name": "Autoencoding Beyond Pixels Using A Learned Similarity MetricIF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: We present an autoencoder that leverages learned representations to better measure similarities in data space.",
      "Link": "http://arxiv.org/pdf/1512.09300v2"
    },
    {
      "Year": "2015",
      "Name": "Deep Multi-scale Video Prediction Beyond Mean Square ErrorIF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: In this work, we train a convolutional network to generate future frames given an input sequence.",
      "Link": "http://arxiv.org/pdf/1511.05440v6"
    },
    {
      "Year": "2015",
      "Name": "Stacked Attention Networks For Image Question AnsweringIF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: This paper presents stacked attention networks (SANs) that learn to answer natural language questions from images.",
      "Link": "http://arxiv.org/pdf/1511.02274v2"
    },
    {
      "Year": "2014",
      "Name": "Adam: A Method For Stochastic OptimizationIF:10Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments.",
      "Link": "http://arxiv.org/pdf/1412.6980v9"
    },
    {
      "Year": "2014",
      "Name": "Conditional Generative Adversarial NetsIF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: In this work we introduce the conditional version of generative adversarial nets, which can be constructed by simply feeding the data, y, we wish to condition on to both the generator and discriminator.",
      "Link": "http://arxiv.org/pdf/1411.1784v1"
    },
    {
      "Year": "2014",
      "Name": "How Transferable Are Features In Deep Neural Networks?IF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: In this paper we experimentally quantify the generality versus specificity of neurons in each layer of a deep convolutional neural network and report a few surprising results.",
      "Link": "http://arxiv.org/pdf/1411.1792v1"
    },
    {
      "Year": "2014",
      "Name": "FitNets: Hints For Thin Deep NetsIF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: In this paper, we extend this idea to allow the training of a student that is deeper and thinner than the teacher, using not only the outputs but also the intermediate representations learned by the teacher as hints to improve the training process and final performance of the student.",
      "Link": "http://arxiv.org/pdf/1412.6550v4"
    },
    {
      "Year": "2014",
      "Name": "Semi-Supervised Learning With Deep Generative ModelsIF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: We revisit the approach to semi-supervised learning with generative models and develop new models that allow for effective generalisation from small labelled data sets to large unlabelled ones.",
      "Link": "http://arxiv.org/pdf/1406.5298v2"
    },
    {
      "Year": "2014",
      "Name": "A Tutorial On Principal Component AnalysisIF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: The goal of this paper is to dispel the magic behind this black box.",
      "Link": "http://arxiv.org/pdf/1404.1100v1"
    },
    {
      "Year": "2014",
      "Name": "NICE: Non-linear Independent Components EstimationIF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: We propose a deep learning framework for modeling complex high-dimensional densities called Non-linear Independent Component Estimation (NICE).",
      "Link": "http://arxiv.org/pdf/1410.8516v6"
    },
    {
      "Year": "2014",
      "Name": "Deep Metric Learning Using Triplet NetworkIF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: In this paper we propose the triplet network model, which aims to learn useful representations by distance comparisons.",
      "Link": "http://arxiv.org/pdf/1412.6622v4"
    },
    {
      "Year": "2014",
      "Name": "SAGA: A Fast Incremental Gradient Method With Support For Non-Strongly Convex Composite ObjectivesIF:8Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: In this work we introduce a new optimisation method called SAGA in the spirit of SAG, SDCA, MISO and SVRG, a set of recently proposed incremental gradient algorithms with fast linear convergence rates.",
      "Link": "http://arxiv.org/pdf/1407.0202v3"
    },
    {
      "Year": "2014",
      "Name": "Collaborative Deep Learning For Recommender SystemsIF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: To address this problem, we generalize recent advances in deep learning from i.i.d. input to non-i.i.d. (CF-based) input and propose in this paper a hierarchical Bayesian model called collaborative deep learning (CDL), which jointly performs deep representation learning for the content information and collaborative filtering for the ratings (feedback) matrix.",
      "Link": "http://arxiv.org/pdf/1409.2944v2"
    },
    {
      "Year": "2014",
      "Name": "Unifying Visual-Semantic Embeddings With Multimodal Neural Language ModelsIF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: Inspired by recent advances in multimodal learning and machine translation, we introduce an encoder-decoder pipeline that learns (a): a multimodal joint embedding space with images and text and (b): a novel language model for decoding distributed representations from our space.",
      "Link": "http://arxiv.org/pdf/1411.2539v1"
    },
    {
      "Year": "2014",
      "Name": "OpenML: Networked Science In Machine LearningIF:8Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: In this paper, we introduce OpenML, a place for machine learning researchers to share and organize data in fine detail, so that they can work more effectively, be more visible, and collaborate with others to tackle harder problems.",
      "Link": "http://arxiv.org/pdf/1407.7722v2"
    },
    {
      "Year": "2014",
      "Name": "The Loss Surfaces Of Multilayer NetworksIF:8Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: We study the connection between the highly non-convex loss function of a simple model of the fully-connected feed-forward neural network and the Hamiltonian of the spherical spin-glass model under the assumptions of: i) variable independence, ii) redundancy in network parametrization, and iii) uniformity.",
      "Link": "http://arxiv.org/pdf/1412.0233v3"
    },
    {
      "Year": "2014",
      "Name": "Multiple Object Recognition With Visual AttentionIF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: We present an attention-based model for recognizing multiple objects in images.",
      "Link": "http://arxiv.org/pdf/1412.7755v2"
    },
    {
      "Year": "2014",
      "Name": "Neural Variational Inference And Learning In Belief NetworksIF:7Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: We propose a fast non-iterative approximate inference method that uses a feedforward network to implement efficient exact sampling from the variational posterior.",
      "Link": "http://arxiv.org/pdf/1402.0030v2"
    },
    {
      "Year": "2014",
      "Name": "Convolutional Neural Networks Over Tree Structures For Programming Language ProcessingIF:7Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: In this paper, we propose a novel tree-based convolutional neural network (TBCNN) for programming language processing, in which a convolution kernel is designed over programs\u2019 abstract syntax trees to capture structural information.",
      "Link": "http://arxiv.org/pdf/1409.5718v2"
    },
    {
      "Year": "2014",
      "Name": "Breaking The Curse Of Dimensionality With Convex Neural NetworksIF:7Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsViewHighlight: By letting the number of hidden units grow unbounded and using classical non-Euclidean regularization tools on the output weights, we provide a detailed theoretical analysis of their generalization performance, with a study of both the approximation and the estimation errors.",
      "Link": "http://arxiv.org/pdf/1412.8690v2"
    },
    {
      "Year": "2014",
      "Name": "Deep Learning With Elastic Averaging SGDIF:7Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: We propose synchronous and asynchronous variants of the new algorithm.",
      "Link": "http://arxiv.org/pdf/1412.6651v8"
    },
    {
      "Year": "2014",
      "Name": "New Insights And Perspectives On The Natural Gradient MethodIF:7Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: In this paper we critically analyze this method and its properties, and show how it can be viewed as a type of approximate 2nd-order optimization method, where the Fisher information matrix can be viewed as an approximation of the Hessian.",
      "Link": "http://arxiv.org/pdf/1412.1193v11"
    },
    {
      "Year": "2014",
      "Name": "Taming The Monster: A Fast And Simple Algorithm For Contextual BanditsIF:6Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: We present a new algorithm for the contextual bandit learning problem, where the learner repeatedly takes one of $K$ actions in response to the observed context, and observes the reward only for that chosen action.",
      "Link": "http://arxiv.org/pdf/1402.0555v2"
    },
    {
      "Year": "2014",
      "Name": "Label Distribution LearningIF:6Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsViewHighlight: This paper proposes six working LDL algorithms in three ways: problem transformation, algorithm adaptation, and specialized algorithm design.",
      "Link": "http://arxiv.org/pdf/1408.6027v2"
    },
    {
      "Year": "2014",
      "Name": "Video (language) Modeling: A Baseline For Generative Models Of Natural VideosIF:6Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: We propose a strong baseline model for unsupervised feature learning using video data.",
      "Link": "http://arxiv.org/pdf/1412.6604v5"
    },
    {
      "Year": "2014",
      "Name": "On The Computational Efficiency Of Training Neural NetworksIF:6Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: In this paper we revisit the computational complexity of training neural networks from a modern perspective.",
      "Link": "http://arxiv.org/pdf/1410.1141v2"
    },
    {
      "Year": "2014",
      "Name": "Guaranteed Matrix Completion Via Non-convex FactorizationIF:6Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsViewHighlight: In this paper, we establish a theoretical guarantee for the factorization formulation to correctly recover the underlying low-rank matrix.",
      "Link": "http://arxiv.org/pdf/1411.8003v3"
    },
    {
      "Year": "2014",
      "Name": "Fastfood: Approximate Kernel Expansions In Loglinear TimeIF:6Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: In this paper, we overcome this difficulty by proposing Fastfood, an approximation that accelerates such computation significantly.",
      "Link": "http://arxiv.org/pdf/1408.3060v1"
    },
    {
      "Year": "2014",
      "Name": "Deep Unfolding: Model-Based Inspiration Of Novel Deep ArchitecturesIF:6Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsViewHighlight: This work aims to obtain the advantages of both approaches.",
      "Link": "http://arxiv.org/pdf/1409.2574v4"
    },
    {
      "Year": "2014",
      "Name": "An Information-Theoretic Analysis Of Thompson SamplingIF:6Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsViewHighlight: We provide an information-theoretic analysis of Thompson sampling that applies across a broad range of online optimization problems in which a decision-maker must learn from partial feedback.",
      "Link": "http://arxiv.org/pdf/1403.5341v2"
    },
    {
      "Year": "2014",
      "Name": "Lazier Than Lazy GreedyIF:6Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsViewHighlight: In this paper, we develop the first linear-time algorithm for maximizing a general monotone submodular function subject to a cardinality constraint.",
      "Link": "http://arxiv.org/pdf/1409.7938v3"
    },
    {
      "Year": "2014",
      "Name": "Differentially Private Empirical Risk Minimization: Efficient Algorithms And Tight Error BoundsIF:6Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: In this paper, we initiate a systematic investigation of differentially private algorithms for convex empirical risk minimization.",
      "Link": "http://arxiv.org/pdf/1405.7085v2"
    },
    {
      "Year": "2013",
      "Name": "Playing Atari With Deep Reinforcement LearningIF:10Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning.",
      "Link": "http://arxiv.org/pdf/1312.5602v1"
    },
    {
      "Year": "2013",
      "Name": "Spectral Networks And Locally Connected Networks On GraphsIF:8Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: In this paper we consider possible generalizations of CNNs to signals defined on more general domains without the action of a translation group.",
      "Link": "http://arxiv.org/pdf/1312.6203v3"
    },
    {
      "Year": "2013",
      "Name": "Estimating Continuous Distributions In Bayesian ClassifiersIF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsViewHighlight: In this paper we abandon the normality assumption and instead use statistical methods for nonparametric density estimation.",
      "Link": "http://arxiv.org/pdf/1302.4964v1"
    },
    {
      "Year": "2013",
      "Name": "Probabilistic Latent Semantic AnalysisIF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: In order to avoid overfitting, we propose a widely applicable generalization of maximum likelihood model fitting by tempered EM.",
      "Link": "http://arxiv.org/pdf/1301.6705v1"
    },
    {
      "Year": "2013",
      "Name": "Estimating Or Propagating Gradients Through Stochastic Neurons For Conditional ComputationIF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: We examine this question, existing approaches, and compare four families of solutions, applicable in different settings.",
      "Link": "http://arxiv.org/pdf/1308.3432v1"
    },
    {
      "Year": "2013",
      "Name": "Do Deep Nets Really Need To Be Deep?IF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: In this extended abstract, we show that shallow feed-forward networks can learn the complex functions previously learned by deep nets and achieve accuracies previously only achievable with deep models.",
      "Link": "http://arxiv.org/pdf/1312.6184v7"
    },
    {
      "Year": "2013",
      "Name": "Deep Learning For Detecting Robotic GraspsIF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsViewHighlight: In this work, we apply a deep learning approach to solve this problem, which avoids time-consuming hand-design of features.",
      "Link": "http://arxiv.org/pdf/1301.3592v6"
    },
    {
      "Year": "2013",
      "Name": "Rao-Blackwellised Particle Filtering For Dynamic Bayesian NetworksIF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsViewHighlight: In this paper, we show how we can exploit the structure of the DBN to increase the efficiency of particle filtering, using a technique known as Rao-Blackwellisation.",
      "Link": "http://arxiv.org/pdf/1301.3853v1"
    },
    {
      "Year": "2013",
      "Name": "Predicting Parameters In Deep LearningIF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsViewHighlight: We demonstrate that there is significant redundancy in the parameterization of several deep learning models.",
      "Link": "http://arxiv.org/pdf/1306.0543v2"
    },
    {
      "Year": "2013",
      "Name": "Gaussian Processes For Big DataIF:8Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: We introduce stochastic variational inference for Gaussian process models.",
      "Link": "http://arxiv.org/pdf/1309.6835v1"
    },
    {
      "Year": "2013",
      "Name": "A Survey On Multi-view LearningIF:8Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsViewHighlight: In recent years, a great many methods of learning from multi-view data by considering the diversity of different views have been proposed.",
      "Link": "http://arxiv.org/pdf/1304.5634v1"
    },
    {
      "Year": "2013",
      "Name": "Stochastic Pooling For Regularization Of Deep Convolutional Neural NetworksIF:8Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: We introduce a simple and effective method for regularizing large convolutional neural networks.",
      "Link": "http://arxiv.org/pdf/1301.3557v1"
    },
    {
      "Year": "2013",
      "Name": "Zero-Shot Learning By Convex Combination Of Semantic EmbeddingsIF:8Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: Several recent publications have proposed methods for mapping images into continuous semantic embedding spaces.",
      "Link": "http://arxiv.org/pdf/1312.5650v3"
    },
    {
      "Year": "2013",
      "Name": "Deep Learning Using Linear Support Vector MachinesIF:8Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: In this paper, we demonstrate a small but consistent advantage of replacing the softmax layer with a linear support vector machine.",
      "Link": "http://arxiv.org/pdf/1306.0239v4"
    },
    {
      "Year": "2013",
      "Name": "Induction Of Selective Bayesian ClassifiersIF:8Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsViewHighlight: In this paper, we examine previous work on the naive Bayesian classifier and review its limitations, which include a sensitivity to correlated features.",
      "Link": "http://arxiv.org/pdf/1302.6828v1"
    },
    {
      "Year": "2013",
      "Name": "The Bayesian Structural EM AlgorithmIF:7Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsViewHighlight: In this paper, I extend Structural EM to deal directly with Bayesian model selection.",
      "Link": "http://arxiv.org/pdf/1301.7373v1"
    },
    {
      "Year": "2013",
      "Name": "A Survey On Metric Learning For Feature Vectors And Structured DataIF:7Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsViewHighlight: This survey paper proposes a systematic review of the metric learning literature, highlighting the pros and cons of each approach.",
      "Link": "http://arxiv.org/pdf/1306.6709v4"
    },
    {
      "Year": "2013",
      "Name": "A Semantic Matching Energy Function For Learning With Multi-relational DataIF:7Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsViewHighlight: In this paper, we present a new neural network architecture designed to embed multi-relational graphs into a flexible continuous vector space in which the original data is kept and enhanced.",
      "Link": "http://arxiv.org/pdf/1301.3485v2"
    },
    {
      "Year": "2013",
      "Name": "Inferring Parameters And Structure Of Latent Variable Models By Variational BayesIF:7Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsViewHighlight: In this paper I present the Variational Bayes framework, which provides a solution to these problems.",
      "Link": "http://arxiv.org/pdf/1301.6676v1"
    },
    {
      "Year": "2013",
      "Name": "Deep Learning Of Representations: Looking ForwardIF:7Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsViewHighlight: Deep learning research aims at discovering learning algorithms that discover multiple levels of distributed representations, with higher levels representing more abstract concepts.",
      "Link": "http://arxiv.org/pdf/1305.0445v2"
    },
    {
      "Year": "2013",
      "Name": "Learning To Optimize Via Posterior SamplingIF:7Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsViewHighlight: Our second theoretical contribution is a Bayesian regret bound for posterior sampling that applies broadly and can be specialized to many model classes.",
      "Link": "http://arxiv.org/pdf/1301.2609v5"
    },
    {
      "Year": "2013",
      "Name": "Learning Bayesian Network Structure From Massive Datasets: The Sparse Candidate AlgorithmIF:7Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsViewHighlight: In this paper, we introduce an algorithm that achieves faster learning by restricting the search space.",
      "Link": "http://arxiv.org/pdf/1301.6696v1"
    },
    {
      "Year": "2013",
      "Name": "One-Class Classification: Taxonomy Of Study And Review Of TechniquesIF:7Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsViewHighlight: In this paper we present a unified view of the general problem of OCC by presenting a taxonomy of study for OCC problems, which is based on the availability of training data, algorithms used and the application domains applied.",
      "Link": "http://arxiv.org/pdf/1312.0049v1"
    },
    {
      "Year": "2013",
      "Name": "An Introductory Study On Time Series Modeling And ForecastingIF:7Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsViewHighlight: The aim of this dissertation work is to present a concise description of some popular time series forecasting models used in practice, with their salient features.",
      "Link": "http://arxiv.org/pdf/1302.6613v1"
    },
    {
      "Year": "2013",
      "Name": "Class Imbalance Problem In Data Mining ReviewIF:7Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsViewHighlight: In this paper systematic study of each approach is define which gives the right direction for research in class imbalance problem.",
      "Link": "http://arxiv.org/pdf/1305.1707v1"
    },
    {
      "Year": "2013",
      "Name": "Generalized Denoising Auto-Encoders As Generative ModelsIF:7Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: We propose here a different attack on the problem, which deals with all these issues: arbitrary (but noisy enough) corruption, arbitrary reconstruction loss (seen as a log-likelihood), handling both discrete and continuous-valued variables, and removing the bias due to non-infinitesimal corruption noise (or non-infinitesimal contractive penalty).",
      "Link": "http://arxiv.org/pdf/1305.6663v4"
    },
    {
      "Year": "2013",
      "Name": "A Time Series Forest For Classification And Feature ExtractionIF:6Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsViewHighlight: We propose a tree ensemble method, referred to as time series forest (TSF), for time series classification.",
      "Link": "http://arxiv.org/pdf/1302.2277v2"
    },
    {
      "Year": "2013",
      "Name": "Learning By TransductionIF:6Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsViewHighlight: We describe a method for predicting a classification of an object given classifications of the objects in the training set, assuming that the pairs object/classification are generated by an i.i.d. process from a continuous probability distribution.",
      "Link": "http://arxiv.org/pdf/1301.7375v1"
    },
    {
      "Year": "2013",
      "Name": "Large-scale Multi-label Learning With Missing LabelsIF:6Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsViewHighlight: In this paper, we directly address both these problems by studying the multi-label problem in a generic empirical risk minimization (ERM) framework.",
      "Link": "http://arxiv.org/pdf/1307.5101v3"
    },
    {
      "Year": "2013",
      "Name": "Comparing Bayesian Network ClassifiersIF:6Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsViewHighlight: In this paper, we empirically evaluate algorithms for learning four types of Bayesian network (BN) classifiers \u2013 Naive-Bayes, tree augmented Naive-Bayes, BN augmented Naive-Bayes and general BNs, where the latter two are learned using two variants of a conditional-independence (CI) based BN-learning algorithm.",
      "Link": "http://arxiv.org/pdf/1301.6684v1"
    },
    {
      "Year": "2012",
      "Name": "Scikit-learn: Machine Learning In PythonIF:10Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems.",
      "Link": "http://arxiv.org/pdf/1201.0490v4"
    },
    {
      "Year": "2012",
      "Name": "Representation Learning: A Review And New PerspectivesIF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data.",
      "Link": "http://arxiv.org/pdf/1206.5538v3"
    },
    {
      "Year": "2012",
      "Name": "ADADELTA: An Adaptive Learning Rate MethodIF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: We present a novel per-dimension learning rate method for gradient descent called ADADELTA.",
      "Link": "http://arxiv.org/pdf/1212.5701v1"
    },
    {
      "Year": "2012",
      "Name": "On The Difficulty Of Training Recurrent Neural NetworksIF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsViewHighlight: In this paper we attempt to improve the understanding of the underlying issues by exploring these problems from an analytical, a geometric and a dynamical systems perspective.",
      "Link": "http://arxiv.org/pdf/1211.5063v2"
    },
    {
      "Year": "2012",
      "Name": "Practical Recommendations For Gradient-based Training Of Deep ArchitecturesIF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: This chapter is meant as a practical guide with recommendations for some of the most commonly used hyper-parameters, in particular in the context of learning algorithms based on back-propagated gradient and gradient-based optimization.",
      "Link": "http://arxiv.org/pdf/1206.5533v2"
    },
    {
      "Year": "2012",
      "Name": "Auto-WEKA: Combined Selection And Hyperparameter Optimization Of Classification AlgorithmsIF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: We consider the problem of simultaneously selecting a learning algorithm and setting its hyperparameters, going beyond previous work that addresses these issues in isolation.",
      "Link": "http://arxiv.org/pdf/1208.3719v2"
    },
    {
      "Year": "2012",
      "Name": "Poisoning Attacks Against Support Vector MachinesIF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: We investigate a family of poisoning attacks against Support Vector Machines (SVM).",
      "Link": "http://arxiv.org/pdf/1206.6389v3"
    },
    {
      "Year": "2012",
      "Name": "Tensor Decompositions For Learning Latent Variable ModelsIF:8Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsViewHighlight: This work considers a computationally and statistically efficient parameter estimation method for a wide class of latent variable models\u2014including Gaussian mixture models, hidden Markov models, and latent Dirichlet allocation\u2014which exploits a certain tensor structure in their low-order observable moments (typically, of second- and third-order).",
      "Link": "http://arxiv.org/pdf/1210.7559v4"
    },
    {
      "Year": "2012",
      "Name": "A Comparative Study Of Efficient Initialization Methods For The K-Means Clustering AlgorithmIF:8Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: Numerous initialization methods have been proposed to address this problem.",
      "Link": "http://arxiv.org/pdf/1209.1960v1"
    },
    {
      "Year": "2012",
      "Name": "Thompson Sampling For Contextual Bandits With Linear PayoffsIF:8Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: In this paper, we design and analyze a generalization of Thompson Sampling algorithm for the stochastic contextual multi-armed bandit problem with linear payoff functions, when the contexts are provided by an adaptive adversary.",
      "Link": "http://arxiv.org/pdf/1209.3352v4"
    },
    {
      "Year": "2012",
      "Name": "An MDP-based Recommender SystemIF:8Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsViewHighlight: We describe our predictive model in detail and evaluate its performance on real data.",
      "Link": "http://arxiv.org/pdf/1301.0600v2"
    },
    {
      "Year": "2012",
      "Name": "Discriminative Probabilistic Models For Relational DataIF:8Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsViewHighlight: In this paper, we present an alternative framework that builds on (conditional) Markov networks and addresses two limitations of the previous approach.",
      "Link": "http://arxiv.org/pdf/1301.0604v1"
    },
    {
      "Year": "2012",
      "Name": "Marginalized Denoising Autoencoders For Domain AdaptationIF:8Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsViewHighlight: In this paper, we propose marginalized SDA (mSDA) that addresses two crucial limitations of SDAs: high computational cost and lack of scalability to high-dimensional features.",
      "Link": "http://arxiv.org/pdf/1206.4683v1"
    },
    {
      "Year": "2012",
      "Name": "Large-Sample Learning Of Bayesian Networks Is NP-HardIF:8Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsViewHighlight: In this paper, we provide new complexity results for algorithms that learn discrete-variable Bayesian networks from data.",
      "Link": "http://arxiv.org/pdf/1212.2468v1"
    },
    {
      "Year": "2012",
      "Name": "Sum-Product Networks: A New Deep ArchitectureIF:7Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: We show that if an SPN is complete and consistent it represents the partition function and all marginals of some graphical model, and give semantics to its nodes.",
      "Link": "http://arxiv.org/pdf/1202.3732v1"
    },
    {
      "Year": "2012",
      "Name": "Generalized Fisher Score For Feature SelectionIF:7Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: In this paper, we present a generalized Fisher score to jointly select features.",
      "Link": "http://arxiv.org/pdf/1202.3725v1"
    },
    {
      "Year": "2012",
      "Name": "A Widely Applicable Bayesian Information CriterionIF:7Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsViewHighlight: In the present paper, we define a widely applicable Bayesian information criterion (WBIC) by the average log likelihood function over the posterior distribution with the inverse temperature $1/\\log n$, where $n$ is the number of training samples.",
      "Link": "http://arxiv.org/pdf/1208.6338v1"
    },
    {
      "Year": "2012",
      "Name": "Modeling Temporal Dependencies In High-Dimensional Sequences: Application To Polyphonic Music Generation And TranscriptionIF:7Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsViewHighlight: We introduce a probabilistic model based on distribution estimators conditioned on a recurrent neural network that is able to discover temporal dependencies in high-dimensional sequences.",
      "Link": "http://arxiv.org/pdf/1206.6392v1"
    },
    {
      "Year": "2012",
      "Name": "On Smoothing And Inference For Topic ModelsIF:7Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: In this paper, we highlight the close connections between these approaches.",
      "Link": "http://arxiv.org/pdf/1205.2662v1"
    },
    {
      "Year": "2012",
      "Name": "Expectation-Propogation For The Generative Aspect ModelIF:7Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsViewHighlight: We develop an alternative approach that leads to higher accuracy at comparable cost.",
      "Link": "http://arxiv.org/pdf/1301.0588v1"
    },
    {
      "Year": "2012",
      "Name": "Kernel-based Conditional Independence Test And Application In Causal DiscoveryIF:7Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: We propose a Kernel-based Conditional Independence test (KCI-test), by constructing an appropriate test statistic and deriving its asymptotic distribution under the null hypothesis of conditional independence.",
      "Link": "http://arxiv.org/pdf/1202.3775v1"
    },
    {
      "Year": "2012",
      "Name": "Stochastic Gradient Descent For Non-smooth Optimization: Convergence Results And Optimal Averaging SchemesIF:7Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsViewHighlight: In this paper, we investigate the performance of SGD without such smoothness assumptions, as well as a running average scheme to convert the SGD iterates to a solution with optimal optimization accuracy.",
      "Link": "http://arxiv.org/pdf/1212.1824v2"
    },
    {
      "Year": "2012",
      "Name": "Algorithms For Learning Kernels Based On Centered AlignmentIF:7Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsViewHighlight: This paper presents new and effective algorithms for learning kernels.",
      "Link": "http://arxiv.org/pdf/1203.0550v3"
    },
    {
      "Year": "2012",
      "Name": "Advances In Optimizing Recurrent NetworksIF:7Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsViewHighlight: Experiments reported here evaluate the use of clipping gradients, spanning longer time ranges with leaky integration, advanced momentum techniques, using more powerful output probability models, and encouraging sparser gradients to help symmetry breaking and credit assignment.",
      "Link": "http://arxiv.org/pdf/1212.0901v2"
    },
    {
      "Year": "2012",
      "Name": "Learning Task Grouping And Overlap In Multi-task LearningIF:7Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsViewHighlight: We propose a framework for multi-task learn- ing that enables one to selectively share the information across the tasks.",
      "Link": "http://arxiv.org/pdf/1206.6417v1"
    },
    {
      "Year": "2012",
      "Name": "Efficiently Inducing Features Of Conditional Random FieldsIF:6Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsViewHighlight: This paper presents a feature induction method for CRFs.",
      "Link": "http://arxiv.org/pdf/1212.2504v1"
    },
    {
      "Year": "2012",
      "Name": "Parallelizing Exploration-Exploitation Tradeoffs With Gaussian Process Bandit OptimizationIF:6Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsViewHighlight: We formalize the task as a multi-armed bandit problem, where the unknown payoff function is sampled from a Gaussian process (GP), and instead of a single arm, in each round we pull a batch of several arms in parallel.",
      "Link": "http://arxiv.org/pdf/1206.6402v1"
    },
    {
      "Year": "2012",
      "Name": "A New Class Of Upper Bounds On The Log Partition FunctionIF:6Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsViewHighlight: We introduce a new class of upper bounds on the log partition function, based on convex combinations of distributions in the exponential domain, that is applicable to an arbitrary undirected graphical model.",
      "Link": "http://arxiv.org/pdf/1301.0610v1"
    },
    {
      "Year": "2012",
      "Name": "A Convex Formulation For Learning Task Relationships In Multi-Task LearningIF:6Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsViewHighlight: In this paper, we propose a regularization formulation for learning the relationships between tasks in multi-task learning.",
      "Link": "http://arxiv.org/pdf/1203.3536v1"
    },
    {
      "Year": "2012",
      "Name": "A Practical Algorithm For Topic Modeling With Provable GuaranteesIF:6Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: In this paper we present an algorithm for topic model inference that is both provable and practical.",
      "Link": "http://arxiv.org/pdf/1212.4777v1"
    },
    {
      "Year": "2011",
      "Name": "Natural Language Processing (almost) From ScratchIF:10Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: We propose a unified neural network architecture and learning algorithm that can be applied to various natural language processing tasks including: part-of-speech tagging, chunking, named entity recognition, and semantic role labeling.",
      "Link": "http://arxiv.org/pdf/1103.0398v1"
    },
    {
      "Year": "2011",
      "Name": "Building High-level Features Using Large Scale Unsupervised LearningIF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: We consider the problem of building high-level, class-specific feature detectors from only unlabeled data.",
      "Link": "http://arxiv.org/pdf/1112.6209v5"
    },
    {
      "Year": "2011",
      "Name": "Optimization With Sparsity-Inducing PenaltiesIF:8Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsViewHighlight: The goal of this paper is to present from a general perspective optimization tools and techniques dedicated to such sparsity-inducing penalties.",
      "Link": "http://arxiv.org/pdf/1108.0775v2"
    },
    {
      "Year": "2011",
      "Name": "Analysis Of Thompson Sampling For The Multi-armed Bandit ProblemIF:8Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsViewHighlight: In this paper, for the first time, we show that Thompson Sampling algorithm achieves logarithmic expected regret for the multi-armed bandit problem.",
      "Link": "http://arxiv.org/pdf/1111.1797v3"
    },
    {
      "Year": "2011",
      "Name": "Domain Adaptation For Statistical ClassifiersIF:8Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsViewHighlight: We present efficient inference algorithms for this special case based on the technique of conditional expectation maximization.",
      "Link": "http://arxiv.org/pdf/1109.6341v1"
    },
    {
      "Year": "2011",
      "Name": "Making Gradient Descent Optimal For Strongly Convex Stochastic OptimizationIF:7Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsViewHighlight: In this paper, we investigate the optimality of SGD in a stochastic setting.",
      "Link": "http://arxiv.org/pdf/1109.5647v7"
    },
    {
      "Year": "2011",
      "Name": "Doubly Robust Policy Evaluation And LearningIF:7Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: In this work, we leverage the strength and overcome the weaknesses of the two approaches by applying the doubly robust technique to the problems of policy evaluation and optimization.",
      "Link": "http://arxiv.org/pdf/1103.4601v2"
    },
    {
      "Year": "2011",
      "Name": "Convergence Rates Of Inexact Proximal-Gradient Methods For Convex OptimizationIF:7Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsViewHighlight: We consider the problem of optimizing the sum of a smooth convex function and a non-smooth convex function using proximal-gradient methods, where an error is present in the calculation of the gradient of the smooth term or in the proximity operator with respect to the non-smooth term.",
      "Link": "http://arxiv.org/pdf/1109.2415v2"
    },
    {
      "Year": "2011",
      "Name": "Learning With Submodular Functions: A Convex Optimization PerspectiveIF:6Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsViewHighlight: In this monograph, we present the theory of submodular functions from a convex analysis perspective, presenting tight links between certain polyhedra, combinatorial optimization and convex optimization problems.",
      "Link": "http://arxiv.org/pdf/1111.6453v2"
    },
    {
      "Year": "2011",
      "Name": "A Unified Framework For Approximating And Clustering DataIF:6Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsViewHighlight: Given a set $F$ of $n$ positive functions over a ground set $X$, we consider the problem of computing $x^*$ that minimizes the expression $\\sum_{f\\in F}f(x)$, over $x\\in X$.",
      "Link": "http://arxiv.org/pdf/1106.1379v4"
    },
    {
      "Year": "2011",
      "Name": "Optimizing Dialogue Management With Reinforcement Learning: Experiments With The NJFun SystemIF:6Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsViewHighlight: This paper presents a reinforcement learning approach for automatically optimizing a dialogue policy, which addresses the technical challenges in applying reinforcement learning to a working dialogue system with human users.",
      "Link": "http://arxiv.org/pdf/1106.0676v1"
    },
    {
      "Year": "2011",
      "Name": "A Reliable Effective Terascale Linear Learning SystemIF:6Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: We present a system and a set of techniques for learning linear predictors with convex losses on terascale datasets, with trillions of features, {The number of features here refers to the number of non-zero entries in the data matrix.}",
      "Link": "http://arxiv.org/pdf/1110.4198v3"
    },
    {
      "Year": "2011",
      "Name": "Evolutionary Algorithms For Reinforcement LearningIF:6Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsViewHighlight: This article focuses on the application of evolutionary algorithms to the reinforcement learning problem, emphasizing alternative policy representations, credit assignment methods, and problem-specific genetic operators.",
      "Link": "http://arxiv.org/pdf/1106.0221v1"
    },
    {
      "Year": "2011",
      "Name": "Revisiting K-means: New Algorithms Via Bayesian NonparametricsIF:6Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsViewHighlight: In this paper, we revisit the k-means clustering algorithm from a Bayesian nonparametric viewpoint.",
      "Link": "http://arxiv.org/pdf/1111.0352v2"
    },
    {
      "Year": "2011",
      "Name": "Budget-Optimal Task Allocation For Reliable Crowdsourcing SystemsIF:6Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsViewHighlight: In this paper, we consider a general model of such crowdsourcing tasks and pose the problem of minimizing the total price (i.e., number of task assignments) that must be paid to achieve a target overall reliability.",
      "Link": "http://arxiv.org/pdf/1110.3564v4"
    },
    {
      "Year": "2011",
      "Name": "Structured Sparsity Through Convex OptimizationIF:6Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsViewHighlight: Sparse estimation methods are aimed at using or obtaining parsimonious representations of data or models.",
      "Link": "http://arxiv.org/pdf/1109.2397v2"
    },
    {
      "Year": "2011",
      "Name": "Better Mini-Batch Algorithms Via Accelerated Gradient MethodsIF:6Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsViewHighlight: Mini-batch algorithms have been proposed as a way to speed-up stochastic convex optimization problems.",
      "Link": "http://arxiv.org/pdf/1106.4574v1"
    },
    {
      "Year": "2011",
      "Name": "Parallel Coordinate Descent For L1-Regularized Loss MinimizationIF:6Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsViewHighlight: We propose Shotgun, a parallel coordinate descent algorithm for minimizing L1-regularized losses.",
      "Link": "http://arxiv.org/pdf/1105.5379v1"
    },
    {
      "Year": "2011",
      "Name": "Risk-Sensitive Reinforcement Learning Applied To Control Under ConstraintsIF:6Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsViewHighlight: In this paper, we consider Markov Decision Processes (MDPs) with error states.",
      "Link": "http://arxiv.org/pdf/1109.2147v1"
    },
    {
      "Year": "2011",
      "Name": "Learning To Order ThingsIF:5Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsViewHighlight: Nevertheless, we describe simple greedy algorithms that are guaranteed to find a good approximation.",
      "Link": "http://arxiv.org/pdf/1105.5464v1"
    },
    {
      "Year": "2011",
      "Name": "Efficient Optimal Learning For Contextual BanditsIF:5Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsViewHighlight: We address the problem of learning in an online setting where the learner repeatedly observes features, selects among a set of actions, and receives reward for the action taken.",
      "Link": "http://arxiv.org/pdf/1106.2369v1"
    },
    {
      "Year": "2011",
      "Name": "Noise Tolerance Under Risk MinimizationIF:5Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsViewHighlight: In this paper we explore noise tolerant learning of classifiers.",
      "Link": "http://arxiv.org/pdf/1109.5231v4"
    },
    {
      "Year": "2011",
      "Name": "Active Ranking Using Pairwise ComparisonsIF:5Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsViewHighlight: This paper examines the problem of ranking a collection of objects using pairwise comparisons (rankings of two objects).",
      "Link": "http://arxiv.org/pdf/1109.3701v2"
    },
    {
      "Year": "2011",
      "Name": "Active Learning With Multiple ViewsIF:5Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsViewHighlight: We focus here on active learning for multi-view domains, in which there are several disjoint subsets of features (views), each of which is sufficient to learn the target concept.",
      "Link": "http://arxiv.org/pdf/1110.1073v1"
    },
    {
      "Year": "2011",
      "Name": "Trading Regret For Efficiency: Online Convex Optimization With Long Term ConstraintsIF:5Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsViewHighlight: In this paper we propose a framework for solving constrained online convex optimization problem.",
      "Link": "http://arxiv.org/pdf/1111.6082v3"
    },
    {
      "Year": "2011",
      "Name": "Learning Symbolic Models Of Stochastic DomainsIF:5Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsViewHighlight: In this article, we work towards the goal of developing agents that can learn to act in complex worlds.",
      "Link": "http://arxiv.org/pdf/1110.2211v1"
    },
    {
      "Year": "2011",
      "Name": "From Bandits To Experts: On The Value Of Side-ObservationsIF:5Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsViewHighlight: We develop practical algorithms with provable regret guarantees, which depend on non-trivial graph-theoretic properties of the information feedback structure.",
      "Link": "http://arxiv.org/pdf/1106.2436v3"
    },
    {
      "Year": "2011",
      "Name": "Accelerating Reinforcement Learning Through Implicit ImitationIF:4Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsViewHighlight: We propose and study a formal model of implicit imitation that can accelerate reinforcement learning dramatically in certain cases.",
      "Link": "http://arxiv.org/pdf/1106.0681v1"
    },
    {
      "Year": "2011",
      "Name": "Trace Lasso: A Trace Norm Regularization For Correlated DesignsIF:4Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsViewHighlight: In this paper, we introduce a new penalty function which takes into account the correlation of the design matrix to stabilize the estimation.",
      "Link": "http://arxiv.org/pdf/1109.1990v1"
    },
    {
      "Year": "2011",
      "Name": "Learning From Labeled And Unlabeled Data: An Empirical Study Across Techniques And DomainsIF:5Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsViewHighlight: In this paper, we present an empirical study of various semi-supervised learning techniques on a variety of datasets.",
      "Link": "http://arxiv.org/pdf/1109.2047v1"
    },
    {
      "Year": "2010",
      "Name": "A Reduction Of Imitation Learning And Structured Prediction To No-Regret Online LearningIF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: In this paper, we propose a new iterative algorithm, which trains a stationary deterministic policy, that can be seen as a no regret algorithm in an online learning setting.",
      "Link": "http://arxiv.org/pdf/1011.0686v3"
    },
    {
      "Year": "2010",
      "Name": "A Contextual-Bandit Approach To Personalized News Article RecommendationIF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: In this work, we model personalized recommendation of news articles as a contextual bandit problem, a principled approach in which a learning algorithm sequentially selects articles to serve users based on contextual information about the users and articles, while simultaneously adapting its article-selection strategy based on user-click feedback to maximize total user clicks.",
      "Link": "http://arxiv.org/pdf/1003.0146v2"
    },
    {
      "Year": "2010",
      "Name": "A Tutorial On Bayesian Optimization Of Expensive Cost Functions, With Application To Active User Modeling And Hierarchical Reinforcement LearningIF:9Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: We present a tutorial on Bayesian optimization, a method of finding the maximum of expensive cost functions.",
      "Link": "http://arxiv.org/pdf/1012.2599v1"
    },
    {
      "Year": "2010",
      "Name": "Asymptotic Equivalence Of Bayes Cross Validation And Widely Applicable Information Criterion In Singular Learning TheoryIF:8Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsViewHighlight: In the present paper, we theoretically compare the Bayes cross-validation loss and the widely applicable information criterion and prove two theorems.",
      "Link": "http://arxiv.org/pdf/1004.2316v2"
    },
    {
      "Year": "2010",
      "Name": "GraphLab: A New Framework For Parallel Machine LearningIF:8Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: By targeting common patterns in ML, we developed GraphLab, which improves upon abstractions like MapReduce by compactly expressing asynchronous iterative algorithms with sparse computational dependencies while ensuring data consistency and achieving a high degree of parallel performance.",
      "Link": "http://arxiv.org/pdf/1006.4990v1"
    },
    {
      "Year": "2010",
      "Name": "Robust PCA Via Outlier PursuitIF:8Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: We present an efficient convex optimization-based algorithm we call Outlier Pursuit, that under some mild assumptions on the uncorrupted points (satisfied, e.g., by the standard generative assumption in PCA problems) recovers the exact optimal low-dimensional subspace, and identifies the corrupted points.",
      "Link": "http://arxiv.org/pdf/1010.4237v2"
    },
    {
      "Year": "2010",
      "Name": "Optimal Distributed Online Prediction Using Mini-BatchesIF:7Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsViewHighlight: In this work, we present the \\emph{distributed mini-batch} algorithm, a method of converting many serial gradient-based online prediction algorithms into distributed algorithms.",
      "Link": "http://arxiv.org/pdf/1012.1367v2"
    },
    {
      "Year": "2010",
      "Name": "Adaptive Submodularity: Theory And Applications In Active Learning And Stochastic OptimizationIF:7Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsViewHighlight: In this paper, we introduce the concept of adaptive submodularity, generalizing submodular set functions to adaptive policies.",
      "Link": "http://arxiv.org/pdf/1003.3967v5"
    },
    {
      "Year": "2010",
      "Name": "Unbiased Offline Evaluation Of Contextual-bandit-based News Article Recommendation AlgorithmsIF:7Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: In this paper, we introduce a \\emph{replay} methodology for contextual bandit algorithm evaluation.",
      "Link": "http://arxiv.org/pdf/1003.5956v2"
    },
    {
      "Year": "2010",
      "Name": "Robustness And GeneralizationIF:6Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsViewHighlight: We derive generalization bounds for learning algorithms based on their robustness: the property that if a testing sample is similar to a training sample, then the testing error is close to the training error.",
      "Link": "http://arxiv.org/pdf/1005.2243v1"
    },
    {
      "Year": "2010",
      "Name": "X-Armed BanditsIF:6Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsViewHighlight: We consider a generalization of stochastic bandits where the set of arms, $\\cX$, is allowed to be a generic measurable space and the mean-payoff function is locally Lipschitz with respect to a dissimilarity function that is known to the decision maker.",
      "Link": "http://arxiv.org/pdf/1001.4475v2"
    },
    {
      "Year": "2010",
      "Name": "Adaptive Bound Optimization For Online Convex OptimizationIF:6Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsViewHighlight: We introduce a new online convex optimization algorithm that adaptively chooses its regularization function based on the loss functions observed so far.",
      "Link": "http://arxiv.org/pdf/1002.4908v2"
    },
    {
      "Year": "2010",
      "Name": "Settling The Polynomial Learnability Of Mixtures Of GaussiansIF:6Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsViewHighlight: In contrast, because pathological scenarios can arise when considering univariate projections of mixtures of more than two Gaussians, the bulk of the work in this paper concerns how to leverage an algorithm for learning univariate mixtures (of many Gaussians) to yield an efficient algorithm for learning in high dimensions.",
      "Link": "http://arxiv.org/pdf/1004.4223v1"
    },
    {
      "Year": "2010",
      "Name": "Contextual Bandit Algorithms With Supervised Learning GuaranteesIF:6Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsViewHighlight: We address the problem of learning in an online, bandit setting where the learner must repeatedly select among $K$ actions, but only receives partial feedback based on its choices.",
      "Link": "http://arxiv.org/pdf/1002.4058v3"
    },
    {
      "Year": "2010",
      "Name": "Application Of K Means Clustering Algorithm For Prediction Of Students Academic PerformanceIF:6Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsViewHighlight: A system for analyzing students results based on cluster analysis and uses standard statistical algorithms to arrange their scores data according to the level of their performance is described.",
      "Link": "http://arxiv.org/pdf/1002.2425v1"
    },
    {
      "Year": "2010",
      "Name": "Trajectory Clustering And An Application To Airspace MonitoringIF:5Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsViewHighlight: This paper presents a framework aimed at monitoring the behavior of aircraft in a given airspace.",
      "Link": "http://arxiv.org/pdf/1001.5007v2"
    },
    {
      "Year": "2010",
      "Name": "Portfolio Allocation For Bayesian OptimizationIF:5Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsViewHighlight: We propose several portfolio strategies, the best of which we call GP-Hedge, and show that this method outperforms the best individual acquisition function.",
      "Link": "http://arxiv.org/pdf/1009.5419v2"
    },
    {
      "Year": "2010",
      "Name": "Distributed Autonomous Online Learning: Regrets And Intrinsic Privacy-Preserving PropertiesIF:5Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsViewHighlight: In this paper, we consider online learning with {\\em distributed} data sources.",
      "Link": "http://arxiv.org/pdf/1006.4039v3"
    },
    {
      "Year": "2010",
      "Name": "Learning From Logged Implicit Exploration DataIF:5Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsViewHighlight: We empirically verify our solution on two reasonably sized sets of real-world data obtained from Yahoo!.",
      "Link": "http://arxiv.org/pdf/1003.0120v2"
    },
    {
      "Year": "2010",
      "Name": "Collaborative Filtering In A Non-Uniform World: Learning With The Weighted Trace NormIF:5Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsViewHighlight: We introduce a weighted version of the trace-norm regularizer that works well also with non-uniform sampling.",
      "Link": "http://arxiv.org/pdf/1002.2780v1"
    },
    {
      "Year": "2010",
      "Name": "Safe Feature Elimination In Sparse Supervised LearningIF:5Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsViewHighlight: We investigate fast methods that allow to quickly eliminate variables (features) in supervised learning problems involving a convex loss function and a $l_1$-norm penalty, leading to a potentially substantial reduction in the number of variables prior to running the supervised learning algorithm.",
      "Link": "http://arxiv.org/pdf/1009.3515v2"
    },
    {
      "Year": "2010",
      "Name": "Polynomial Learning Of Distribution FamiliesIF:5Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsViewHighlight: To estimate parameters of a Gaussian mixture distribution in high dimensions, we provide a deterministic algorithm for dimensionality reduction.",
      "Link": "http://arxiv.org/pdf/1004.4864v1"
    },
    {
      "Year": "2010",
      "Name": "An Inverse Power Method For Nonlinear Eigenproblems With Applications In 1-Spectral Clustering And Sparse PCAIF:5Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsRelated CodeViewHighlight: In this paper we show that a certain class of constrained optimization problems with nonquadratic objective and constraints can be understood as nonlinear eigenproblems.",
      "Link": "http://arxiv.org/pdf/1012.0774v1"
    },
    {
      "Year": "2010",
      "Name": "Near-Optimal Bayesian Active Learning With Noisy ObservationsIF:5Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsViewHighlight: We tackle the fundamental problem of Bayesian active learning with noise, where we need to adaptively select from a number of expensive tests in order to identify an unknown hypothesis sampled from a known prior distribution.",
      "Link": "http://arxiv.org/pdf/1010.3091v2"
    },
    {
      "Year": "2010",
      "Name": "A CHAID Based Performance Prediction Model In Educational Data MiningIF:5Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsViewHighlight: As this academic performance is influenced by many factors, it is essential to develop predictive data mining model for students\u2019 performance so as to identify the slow learners and study the influence of the dominant factors on their academic performance.",
      "Link": "http://arxiv.org/pdf/1002.1144v1"
    },
    {
      "Year": "2010",
      "Name": "Manifold Elastic Net: A Unified Framework For Sparse Dimension ReductionIF:4Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsViewHighlight: In this paper, we proposed the manifold elastic net or MEN for short.",
      "Link": "http://arxiv.org/pdf/1007.3564v3"
    },
    {
      "Year": "2010",
      "Name": "Sparse Inverse Covariance Selection Via Alternating Linearization MethodsIF:4Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsViewHighlight: In this paper, we propose a first-order method based on an alternating linearization technique that exploits the problem\u2019s special structure; in particular, the subproblems solved in each iteration have closed-form solutions.",
      "Link": "http://arxiv.org/pdf/1011.0097v1"
    },
    {
      "Year": "2010",
      "Name": "Supervised Classification Performance Of Multispectral ImagesIF:4Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsViewHighlight: Here we compare the different classification methods and their performances.",
      "Link": "http://arxiv.org/pdf/1002.4046v1"
    },
    {
      "Year": "2010",
      "Name": "Agnostic Active Learning Without ConstraintsIF:5Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsViewHighlight: We present and analyze an agnostic active learning algorithm that works without keeping a version space.",
      "Link": "http://arxiv.org/pdf/1006.2588v1"
    },
    {
      "Year": "2010",
      "Name": "Network Flow Algorithms For Structured SparsityIF:4Related PapersRelated PatentsRelated GrantsRelated VenuesRelated ExpertsViewHighlight: We propose an efficient procedure which computes its solution exactly in polynomial time.",
      "Link": "http://arxiv.org/pdf/1008.5209v1"
    }
  ]
}